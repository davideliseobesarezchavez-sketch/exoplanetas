{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a3f3c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# webapp/app.py - VERSI√ìN CON RUTAS CORREGIDAS\n",
    "!pip install streamlit\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OBTENER LA RUTA CORRECTA DEL PROYECTO\n",
    "def get_project_root():\n",
    "    \"\"\"Obtener la ruta ra√≠z del proyecto de forma confiable\"\"\"\n",
    "    current_file = os.path.abspath(__file__)\n",
    "    project_root = os.path.dirname(os.path.dirname(current_file))\n",
    "    return project_root\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "\n",
    "class ExoplanetDataProcessor:\n",
    "    \"\"\"Procesador de datos para los datasets reales de la NASA\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def load_real_data(self):\n",
    "        \"\"\"Cargar los datasets reales de la NASA con rutas corregidas\"\"\"\n",
    "        try:\n",
    "            # Usar rutas absolutas desde la ra√≠z del proyecto\n",
    "            data_dir = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "            \n",
    "            st.info(f\"üîç Buscando datos en: {data_dir}\")\n",
    "            \n",
    "            # Listar archivos en el directorio\n",
    "            if os.path.exists(data_dir):\n",
    "                files = os.listdir(data_dir)\n",
    "                st.info(f\"üìÅ Archivos encontrados en data/raw/: {files}\")\n",
    "            else:\n",
    "                st.error(f\"‚ùå No existe el directorio: {data_dir}\")\n",
    "                return None, None, None\n",
    "            \n",
    "            # Construir rutas completas\n",
    "            kepler_path = os.path.join(data_dir, 'kepler.csv')\n",
    "            k2_path = os.path.join(data_dir, 'k2.csv') \n",
    "            tess_path = os.path.join(data_dir, 'tess.csv')\n",
    "            \n",
    "            st.info(f\"üìä Intentando cargar:\\n- {kepler_path}\\n- {k2_path}\\n- {tess_path}\")\n",
    "            \n",
    "            # Verificar que los archivos existen\n",
    "            if not os.path.exists(kepler_path):\n",
    "                st.error(f\"‚ùå No existe: {kepler_path}\")\n",
    "                # Buscar archivos similares\n",
    "                csv_files = [f for f in files if f.endswith('.csv')]\n",
    "                if csv_files:\n",
    "                    st.info(f\"üìÑ Archivos CSV disponibles: {csv_files}\")\n",
    "                return None, None, None\n",
    "            \n",
    "            # Cargar los archivos reales\n",
    "            kepler_df = pd.read_csv(kepler_path)\n",
    "            k2_df = pd.read_csv(k2_path) if os.path.exists(k2_path) else None\n",
    "            tess_df = pd.read_csv(tess_path) if os.path.exists(tess_path) else None\n",
    "            \n",
    "            st.success(f\"‚úÖ Kepler cargado: {len(kepler_df)} registros\")\n",
    "            if k2_df is not None:\n",
    "                st.success(f\"‚úÖ K2 cargado: {len(k2_df)} registros\")\n",
    "            if tess_df is not None:\n",
    "                st.success(f\"‚úÖ TESS cargado: {len(tess_df)} registros\")\n",
    "            \n",
    "            return kepler_df, k2_df, tess_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error cargando datasets: {e}\")\n",
    "            return None, None, None\n",
    "    \n",
    "    def preprocess_kepler(self, df):\n",
    "        \"\"\"Preprocesar datos Kepler reales - VERSI√ìN MEJORADA\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        st.info(\"üîß Procesando datos Kepler...\")\n",
    "        \n",
    "        # Mostrar columnas disponibles\n",
    "        st.write(f\"üìã Columnas en Kepler: {list(df_clean.columns)}\")\n",
    "        \n",
    "        # Verificar si existe la columna de target\n",
    "        if 'koi_disposition' not in df_clean.columns:\n",
    "            st.error(\"‚ùå No se encuentra la columna 'koi_disposition' en Kepler\")\n",
    "            st.info(\"Las columnas disponibles son:\")\n",
    "            st.write(list(df_clean.columns))\n",
    "            return df_clean\n",
    "        \n",
    "        # Eliminar columnas no √∫tiles (basado en el paper)\n",
    "        columns_to_drop = ['kepid', 'kepoi_name', 'kepler_name', 'koi_pdisposition', 'koi_score']\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df_clean.columns]\n",
    "        \n",
    "        if columns_to_drop:\n",
    "            df_clean = df_clean.drop(columns=columns_to_drop)\n",
    "            st.write(f\"üóëÔ∏è Columnas eliminadas: {columns_to_drop}\")\n",
    "        \n",
    "        # Mostrar valores √∫nicos en la columna de disposici√≥n\n",
    "        st.write(f\"üéØ Valores en koi_disposition: {df_clean['koi_disposition'].unique()}\")\n",
    "        \n",
    "        # Filtrar solo confirmed, candidate y false positive\n",
    "        valid_dispositions = ['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE']\n",
    "        mask = df_clean['koi_disposition'].isin(valid_dispositions)\n",
    "        df_clean = df_clean[mask]\n",
    "        \n",
    "        st.write(f\"üìä Distribuci√≥n despu√©s de filtrar: {df_clean['koi_disposition'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Crear target binario\n",
    "        df_clean['target'] = df_clean['koi_disposition'].map({\n",
    "            'CONFIRMED': 1, \n",
    "            'CANDIDATE': 1,\n",
    "            'FALSE POSITIVE': 0\n",
    "        })\n",
    "        \n",
    "        # A√±adir identificador de misi√≥n\n",
    "        df_clean['mission'] = 'kepler'\n",
    "        \n",
    "        st.success(f\"‚úÖ Kepler procesado: {len(df_clean)} registros\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def preprocess_k2(self, df):\n",
    "        \"\"\"Preprocesar datos K2 reales\"\"\"\n",
    "        if df is None:\n",
    "            st.warning(\"‚ö†Ô∏è Dataset K2 no disponible\")\n",
    "            return None\n",
    "            \n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        st.info(\"üîß Procesando datos K2...\")\n",
    "        st.write(f\"üìã Columnas en K2: {list(df_clean.columns)}\")\n",
    "        \n",
    "        # Verificar columnas necesarias\n",
    "        if 'disposition' not in df_clean.columns:\n",
    "            st.error(\"‚ùå No se encuentra la columna 'disposition' en K2\")\n",
    "            return None\n",
    "        \n",
    "        # Filtrar solo confirmed y candidate\n",
    "        df_clean = df_clean[df_clean['disposition'].isin(['CONFIRMED', 'CANDIDATE'])]\n",
    "        \n",
    "        # Target binario\n",
    "        df_clean['target'] = df_clean['disposition'].map({\n",
    "            'CONFIRMED': 1,\n",
    "            'CANDIDATE': 1\n",
    "        })\n",
    "        \n",
    "        # Identificador de misi√≥n\n",
    "        df_clean['mission'] = 'k2'\n",
    "        \n",
    "        st.success(f\"‚úÖ K2 procesado: {len(df_clean)} registros\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def preprocess_tess(self, df):\n",
    "        \"\"\"Preprocesar datos TESS reales\"\"\"\n",
    "        if df is None:\n",
    "            st.warning(\"‚ö†Ô∏è Dataset TESS no disponible\")\n",
    "            return None\n",
    "            \n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        st.info(\"üîß Procesando datos TESS...\")\n",
    "        st.write(f\"üìã Columnas en TESS: {list(df_clean.columns)}\")\n",
    "        \n",
    "        # Verificar columnas necesarias\n",
    "        if 'tfopwg_disp' not in df_clean.columns:\n",
    "            st.error(\"‚ùå No se encuentra la columna 'tfopwg_disp' en TESS\")\n",
    "            return None\n",
    "        \n",
    "        # Mapear disposiciones de TESS\n",
    "        disposition_mapping = {\n",
    "            'PC': 1, 'KP': 1, 'APC': 1,  # Positivos\n",
    "            'FP': 0, 'FA': 0  # Negativos\n",
    "        }\n",
    "        \n",
    "        df_clean['target'] = df_clean['tfopwg_disp'].map(disposition_mapping)\n",
    "        df_clean = df_clean.dropna(subset=['target'])\n",
    "        \n",
    "        # Identificador de misi√≥n\n",
    "        df_clean['mission'] = 'tess'\n",
    "        \n",
    "        st.success(f\"‚úÖ TESS procesado: {len(df_clean)} registros\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Preparar caracter√≠sticas para el modelo - VERSI√ìN FLEXIBLE\"\"\"\n",
    "        if df is None or len(df) == 0:\n",
    "            st.error(\"‚ùå No hay datos para preparar caracter√≠sticas\")\n",
    "            return None, None, None\n",
    "            \n",
    "        st.info(\"üîß Preparando caracter√≠sticas...\")\n",
    "        \n",
    "        # Posibles nombres de columnas en diferentes datasets\n",
    "        possible_features = {\n",
    "            'orbital_period': ['koi_period', 'pl_orbper', 'period'],\n",
    "            'transit_duration': ['koi_duration', 'pl_trandurh', 'duration'],\n",
    "            'transit_depth': ['koi_depth', 'pl_trandep', 'depth'], \n",
    "            'planet_radius': ['koi_prad', 'pl_rade', 'radius'],\n",
    "            'equilibrium_temp': ['koi_teq', 'pl_eqt', 'teq'],\n",
    "            'insolation_flux': ['koi_insol', 'pl_insol', 'insol'],\n",
    "            'stellar_teff': ['koi_steff', 'st_teff', 'teff'],\n",
    "            'stellar_logg': ['koi_slogg', 'st_logg', 'logg'],\n",
    "            'stellar_radius': ['koi_srad', 'st_rad', 'srad']\n",
    "        }\n",
    "        \n",
    "        # Encontrar las columnas disponibles\n",
    "        available_columns = []\n",
    "        for feature_name, possible_names in possible_features.items():\n",
    "            for name in possible_names:\n",
    "                if name in df.columns:\n",
    "                    available_columns.append(name)\n",
    "                    break\n",
    "        \n",
    "        st.write(f\"üìä Columnas num√©ricas encontradas: {available_columns}\")\n",
    "        \n",
    "        if not available_columns:\n",
    "            st.error(\"‚ùå No se encontraron columnas num√©ricas para entrenar\")\n",
    "            return None, None, None\n",
    "        \n",
    "        X = df[available_columns].copy()\n",
    "        y = df['target'].values\n",
    "        \n",
    "        st.write(f\"üìä Shape de X: {X.shape}, Shape de y: {y.shape}\")\n",
    "        \n",
    "        # Manejar valores missing\n",
    "        missing_before = X.isnull().sum().sum()\n",
    "        X = X.fillna(X.median())\n",
    "        missing_after = X.isnull().sum().sum()\n",
    "        \n",
    "        st.write(f\"üîß Valores missing: {missing_before} antes, {missing_after} despu√©s\")\n",
    "        \n",
    "        # Escalar caracter√≠sticas\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.feature_names = available_columns\n",
    "        \n",
    "        st.success(f\"‚úÖ Caracter√≠sticas preparadas: {X_scaled.shape}\")\n",
    "        \n",
    "        return X_scaled, y, available_columns\n",
    "\n",
    "class RealExoplanetModel:\n",
    "    \"\"\"Modelo real para entrenamiento con datos de la NASA\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.accuracy = 0\n",
    "        self.feature_importance = None\n",
    "    \n",
    "    def create_ensemble(self):\n",
    "        \"\"\"Crear ensemble con los algoritmos del paper\"\"\"\n",
    "        base_models = [\n",
    "            ('random_forest', RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                min_samples_split=5,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('extra_trees', ExtraTreesClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('xgboost', XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                random_state=42\n",
    "            )),\n",
    "            ('lightgbm', LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        ensemble = StackingClassifier(\n",
    "            estimators=base_models,\n",
    "            final_estimator=LogisticRegression(),\n",
    "            cv=3,  # Reducido para mayor velocidad\n",
    "            passthrough=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return ensemble\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Entrenar el modelo real\"\"\"\n",
    "        if X is None or y is None:\n",
    "            st.error(\"‚ùå No hay datos para entrenar\")\n",
    "            return None\n",
    "            \n",
    "        st.info(\"ü§ñ Iniciando entrenamiento del ensemble...\")\n",
    "        \n",
    "        self.model = self.create_ensemble()\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "        # Calcular accuracy en entrenamiento\n",
    "        y_pred = self.model.predict(X)\n",
    "        self.accuracy = accuracy_score(y, y_pred)\n",
    "        \n",
    "        st.write(f\"üìà Accuracy en entrenamiento: {self.accuracy:.2%}\")\n",
    "        \n",
    "        # Calcular importancia de caracter√≠sticas\n",
    "        self._calculate_feature_importance(X.shape[1])\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def _calculate_feature_importance(self, n_features):\n",
    "        \"\"\"Calcular importancia de caracter√≠sticas promediada\"\"\"\n",
    "        importances = np.zeros(n_features)\n",
    "        \n",
    "        for name, model in self.model.named_estimators_.items():\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances += model.feature_importances_\n",
    "        \n",
    "        if len(self.model.named_estimators_) > 0:\n",
    "            self.feature_importance = importances / len(self.model.named_estimators_)\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Guardar modelo entrenado\"\"\"\n",
    "        if self.model:\n",
    "            # Asegurar que el directorio existe\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            joblib.dump(self.model, filepath)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Cargar modelo entrenado\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(filepath):\n",
    "                self.model = joblib.load(filepath)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error cargando modelo: {e}\")\n",
    "        return False\n",
    "\n",
    "class ExoplanetDetectorApp:\n",
    "    def __init__(self):\n",
    "        self.model = RealExoplanetModel()\n",
    "        self.data_processor = ExoplanetDataProcessor()\n",
    "        self.model_trained = False\n",
    "        \n",
    "    def render_sidebar(self):\n",
    "        \"\"\"Barra lateral de navegaci√≥n - ACTUALIZADA\"\"\"\n",
    "        st.sidebar.title(\"üî≠ NASA Exoplanet Detector - REAL\")\n",
    "        st.sidebar.markdown(\"---\")\n",
    "        \n",
    "        page = st.sidebar.radio(\"Navegaci√≥n\", [\n",
    "            \"üè† Inicio\", \n",
    "            \"üöÄ Entrenar Modelo REAL\",\n",
    "            \"ü§ñ Clasificar Exoplanetas\",\n",
    "            \"üì¶ Clasificaci√≥n por Lotes\",  # ¬°NUEVA OPCI√ìN!\n",
    "            \"üìä An√°lisis de Datos REAL\",\n",
    "            \"üíæ Modelos Guardados\"\n",
    "        ])\n",
    "        \n",
    "        st.sidebar.markdown(\"---\")\n",
    "        st.sidebar.info(\n",
    "            \"Sistema REAL con datos de Kepler, K2 y TESS de la NASA\"\n",
    "        )\n",
    "        \n",
    "        return page\n",
    "\n",
    "    def render_home(self):\n",
    "        \"\"\"P√°gina de inicio\"\"\"\n",
    "        st.title(\"ü™ê NASA Exoplanet Detection AI - SISTEMA REAL\")\n",
    "        \n",
    "        col1, col2 = st.columns([2, 1])\n",
    "        \n",
    "        with col1:\n",
    "            st.markdown(\"\"\"\n",
    "            ### Sistema REAL de Detecci√≥n de Exoplanetas\n",
    "            \n",
    "            **Caracter√≠sticas IMPLEMENTADAS:**\n",
    "            - ‚úÖ **Entrenamiento REAL** con datos de la NASA\n",
    "            - ‚úÖ **Modelos PERSISTENTES** que se guardan en disco\n",
    "            - ‚úÖ **Datos REALES** Kepler, K2 y TESS\n",
    "            - ‚úÖ **Ensemble Stacking** como en el paper cient√≠fico\n",
    "            - ‚úÖ **Guardado/Auto-carga** de modelos\n",
    "            \n",
    "            **Para comenzar:**\n",
    "            1. Verifica que tus archivos CSV est√©n en `data/raw/`\n",
    "            2. Ve a **'Entrenar Modelo REAL'**\n",
    "            3. ¬°El sistema detectar√° autom√°ticamente tus datos!\n",
    "            \"\"\")\n",
    "        \n",
    "        with col2:\n",
    "            st.image(\"https://www.nasa.gov/sites/default/files/thumbnails/image/kepler_all_planets_art.jpg\", \n",
    "                    use_column_width=True,\n",
    "                    caption=\"Datos REALES de la NASA\")\n",
    "        \n",
    "        # Verificar estructura de archivos\n",
    "        st.subheader(\"üîç Verificaci√≥n de Archivos\")\n",
    "        \n",
    "        data_dir = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "        if os.path.exists(data_dir):\n",
    "            files = os.listdir(data_dir)\n",
    "            csv_files = [f for f in files if f.endswith('.csv')]\n",
    "            \n",
    "            if csv_files:\n",
    "                st.success(f\"‚úÖ Directorio data/raw/ encontrado\")\n",
    "                st.write(f\"üìÑ Archivos CSV: {csv_files}\")\n",
    "            else:\n",
    "                st.warning(f\"‚ö†Ô∏è Directorio existe pero no hay archivos CSV\")\n",
    "        else:\n",
    "            st.error(f\"‚ùå No existe el directorio: {data_dir}\")\n",
    "            st.info(\"\"\"\n",
    "            **Soluci√≥n:**\n",
    "            1. Crea la carpeta `data/raw/` en tu proyecto\n",
    "            2. Coloca all√≠ tus archivos `kepler.csv`, `k2.csv`, `tess.csv`\n",
    "            3. Recarga esta p√°gina\n",
    "            \"\"\")\n",
    "        \n",
    "        # Verificar si hay modelo entrenado\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        if os.path.exists(model_path):\n",
    "            st.success(\"‚úÖ **Modelo entrenado disponible** - Puedes usarlo en 'Clasificar Exoplanetas'\")\n",
    "            if self.model.load_model(model_path):\n",
    "                st.metric(\"Accuracy del Modelo\", f\"{self.model.accuracy:.1%}\")\n",
    "                self.model_trained = True\n",
    "        else:\n",
    "            st.warning(\"‚ö†Ô∏è **No hay modelo entrenado** - Ve a 'Entrenar Modelo REAL' para comenzar\")\n",
    "\n",
    "    def render_real_training(self):\n",
    "        \"\"\"P√°gina de entrenamiento REAL con datos de la NASA\"\"\"\n",
    "        st.title(\"üöÄ Entrenamiento REAL con Datos NASA\")\n",
    "        \n",
    "        st.info(\"\"\"\n",
    "        **Entrenamiento REAL del modelo Ensemble** usando tus datasets de:\n",
    "        - Kepler.csv (datos reales)\n",
    "        - K2.csv (datos reales) \n",
    "        - TESS.csv (datos reales)\n",
    "        \n",
    "        El modelo entrenado se guardar√° autom√°ticamente y estar√° disponible para clasificaci√≥n.\n",
    "        \"\"\")\n",
    "        \n",
    "        if st.button(\"üéØ Iniciar Entrenamiento REAL\", type=\"primary\"):\n",
    "            with st.spinner(\"Cargando y procesando datos REALES de la NASA...\"):\n",
    "                try:\n",
    "                    # Cargar datos reales\n",
    "                    kepler_df, k2_df, tess_df = self.data_processor.load_real_data()\n",
    "                    \n",
    "                    if kepler_df is None:\n",
    "                        st.error(\"\"\"\n",
    "                        ‚ùå **No se pudieron cargar los datasets**\n",
    "                        \n",
    "                        **Posibles soluciones:**\n",
    "                        1. Verifica que los archivos est√©n en `data/raw/`\n",
    "                        2. Aseg√∫rate de que se llamen `kepler.csv`, `k2.csv`, `tess.csv`\n",
    "                        3. Verifica que los archivos no est√©n corruptos\n",
    "                        \"\"\")\n",
    "                        return\n",
    "                    \n",
    "                    # Mostrar informaci√≥n de los datasets\n",
    "                    st.subheader(\"üìä Datasets Cargados\")\n",
    "                    col1, col2, col3 = st.columns(3)\n",
    "                    \n",
    "                    with col1:\n",
    "                        st.metric(\"Kepler\", f\"{len(kepler_df):,} registros\")\n",
    "                    with col2:\n",
    "                        k2_count = len(k2_df) if k2_df is not None else 0\n",
    "                        st.metric(\"K2\", f\"{k2_count:,} registros\")\n",
    "                    with col3:\n",
    "                        tess_count = len(tess_df) if tess_df is not None else 0\n",
    "                        st.metric(\"TESS\", f\"{tess_count:,} registros\")\n",
    "                    \n",
    "                    # Procesar datos\n",
    "                    st.subheader(\"üîß Procesando Datos...\")\n",
    "                    \n",
    "                    # Preprocesar Kepler\n",
    "                    kepler_processed = self.data_processor.preprocess_kepler(kepler_df)\n",
    "                    if kepler_processed is None:\n",
    "                        return\n",
    "                    \n",
    "                    # Preprocesar K2 y TESS si est√°n disponibles\n",
    "                    datasets_to_process = [kepler_processed]\n",
    "                    \n",
    "                    if k2_df is not None:\n",
    "                        k2_processed = self.data_processor.preprocess_k2(k2_df)\n",
    "                        if k2_processed is not None:\n",
    "                            datasets_to_process.append(k2_processed)\n",
    "                    \n",
    "                    if tess_df is not None:\n",
    "                        tess_processed = self.data_processor.preprocess_tess(tess_df)\n",
    "                        if tess_processed is not None:\n",
    "                            datasets_to_process.append(tess_processed)\n",
    "                    \n",
    "                    # Unificar datos (filtrar None values)\n",
    "                    datasets_to_process = [d for d in datasets_to_process if d is not None]\n",
    "                    if not datasets_to_process:\n",
    "                        st.error(\"‚ùå No hay datos v√°lidos para procesar\")\n",
    "                        return\n",
    "                    \n",
    "                    unified_data = pd.concat(datasets_to_process, ignore_index=True)\n",
    "                    \n",
    "                    st.success(f\"‚úÖ Datos unificados: {len(unified_data):,} muestras\")\n",
    "                    \n",
    "                    # Preparar caracter√≠sticas\n",
    "                    X, y, feature_names = self.data_processor.prepare_features(unified_data)\n",
    "                    \n",
    "                    if X is None:\n",
    "                        st.error(\"‚ùå No se pudieron preparar las caracter√≠sticas\")\n",
    "                        return\n",
    "                    \n",
    "                    # Entrenar modelo\n",
    "                    st.subheader(\"ü§ñ Entrenando Modelo Ensemble...\")\n",
    "                    trained_model = self.model.train(X, y)\n",
    "                    \n",
    "                    if trained_model is None:\n",
    "                        st.error(\"‚ùå Error en el entrenamiento\")\n",
    "                        return\n",
    "                    \n",
    "                    # Guardar modelo\n",
    "                    models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "                    model_path = os.path.join(models_dir, 'real_ensemble_model.pkl')\n",
    "                    \n",
    "                    model_saved = self.model.save_model(model_path)\n",
    "                    \n",
    "                    if model_saved:\n",
    "                        # Guardar tambi√©n el preprocesador y feature names\n",
    "                        processor_path = os.path.join(models_dir, 'data_processor.pkl')\n",
    "                        features_path = os.path.join(models_dir, 'feature_names.pkl')\n",
    "                        \n",
    "                        joblib.dump(self.data_processor, processor_path)\n",
    "                        joblib.dump(feature_names, features_path)\n",
    "                        \n",
    "                        st.success(\"‚úÖ Modelo entrenado y guardado exitosamente!\")\n",
    "                        self.model_trained = True\n",
    "                        \n",
    "                        # Mostrar resultados\n",
    "                        st.subheader(\"üìà Resultados del Entrenamiento\")\n",
    "                        col1, col2, col3, col4 = st.columns(4)\n",
    "                        \n",
    "                        with col1:\n",
    "                            st.metric(\"Accuracy\", f\"{self.model.accuracy:.2%}\")\n",
    "                        with col2:\n",
    "                            st.metric(\"Muestras\", f\"{X.shape[0]:,}\")\n",
    "                        with col3:\n",
    "                            st.metric(\"Caracter√≠sticas\", X.shape[1])\n",
    "                        with col4:\n",
    "                            st.metric(\"Algoritmos\", \"4 Ensemble\")\n",
    "                        \n",
    "                        # Importancia de caracter√≠sticas\n",
    "                        if self.model.feature_importance is not None:\n",
    "                            st.subheader(\"üîç Importancia de Caracter√≠sticas\")\n",
    "                            importance_df = pd.DataFrame({\n",
    "                                'Caracter√≠stica': feature_names,\n",
    "                                'Importancia': self.model.feature_importance\n",
    "                            }).sort_values('Importancia', ascending=False)\n",
    "                            \n",
    "                            fig = px.bar(\n",
    "                                importance_df.head(10),\n",
    "                                x='Importancia',\n",
    "                                y='Caracter√≠stica',\n",
    "                                title='Top 10 Caracter√≠sticas M√°s Importantes',\n",
    "                                orientation='h'\n",
    "                            )\n",
    "                            st.plotly_chart(fig, use_container_width=True)\n",
    "                    \n",
    "                    st.balloons()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    st.error(f\"‚ùå Error durante el entrenamiento: {str(e)}\")\n",
    "                    import traceback\n",
    "                    st.code(traceback.format_exc())\n",
    "\n",
    "    # ... (el resto de las funciones se mantienen igual que antes)\n",
    "    def render_real_classification(self):\n",
    "        \"\"\"Clasificaci√≥n con modelo REAL entrenado - VERSI√ìN CORREGIDA\"\"\"\n",
    "        st.title(\"ü§ñ Clasificaci√≥n con Modelo REAL\")\n",
    "        \n",
    "        # Verificar si hay modelo entrenado\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        if not os.path.exists(model_path):\n",
    "            st.warning(\"\"\"\n",
    "            ‚ö†Ô∏è **No hay modelo entrenado**\n",
    "            \n",
    "            Para usar el clasificador REAL:\n",
    "            1. Ve a la pesta√±a **'Entrenar Modelo REAL'**\n",
    "            2. Entrena el modelo con tus datos de la NASA\n",
    "            3. Regresa aqu√≠ para clasificar candidatos\n",
    "            \"\"\")\n",
    "            return\n",
    "        \n",
    "        # Cargar modelo\n",
    "        if not self.model_trained:\n",
    "            if self.model.load_model(model_path):\n",
    "                self.model_trained = True\n",
    "                st.success(\"‚úÖ Modelo REAL cargado exitosamente\")\n",
    "                \n",
    "                # Mostrar informaci√≥n de caracter√≠sticas\n",
    "                features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "                if os.path.exists(features_path):\n",
    "                    feature_names = joblib.load(features_path)\n",
    "                    st.info(f\"üîç El modelo espera {len(feature_names)} caracter√≠sticas: {', '.join(feature_names)}\")\n",
    "            else:\n",
    "                st.error(\"‚ùå Error cargando el modelo\")\n",
    "                return\n",
    "        \n",
    "        st.info(\"\"\"\n",
    "        üîç **Clasificador REAL**: Introduce los 9 par√°metros astron√≥micos que el modelo espera.\n",
    "        \"\"\")\n",
    "        \n",
    "        with st.form(\"real_classification_form\"):\n",
    "            st.subheader(\"üìê Par√°metros del Candidato - 9 CARACTER√çSTICAS REQUERIDAS\")\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                koi_period = st.number_input(\"Per√≠odo Orbital - koi_period (d√≠as)\", \n",
    "                                        min_value=0.1, max_value=1000.0, value=10.0,\n",
    "                                        help=\"Tiempo orbital del planeta\")\n",
    "                \n",
    "                koi_duration = st.number_input(\"Duraci√≥n Tr√°nsito - koi_duration (horas)\", \n",
    "                                            min_value=0.1, max_value=24.0, value=3.0,\n",
    "                                            help=\"Duraci√≥n del tr√°nsito\")\n",
    "                \n",
    "                koi_depth = st.number_input(\"Profundidad Tr√°nsito - koi_depth (ppm)\", \n",
    "                                        min_value=1, max_value=100000, value=500,\n",
    "                                        help=\"Disminuci√≥n de brillo durante tr√°nsito\")\n",
    "                \n",
    "                koi_prad = st.number_input(\"Radio Planetario - koi_prad (Radios Tierra)\", \n",
    "                                        min_value=0.1, max_value=50.0, value=2.0,\n",
    "                                        help=\"Radio del planeta en unidades terrestres\")\n",
    "                \n",
    "                koi_teq = st.number_input(\"Temperatura Equilibrio - koi_teq (K)\", \n",
    "                                        min_value=100, max_value=5000, value=500,\n",
    "                                        help=\"Temperatura de equilibrio del planeta\")\n",
    "            \n",
    "            with col2:\n",
    "                koi_insol = st.number_input(\"Flujo de Insolaci√≥n - koi_insol\", \n",
    "                                        min_value=0.1, max_value=10000.0, value=100.0,\n",
    "                                        help=\"Flujo de radiaci√≥n recibido\")\n",
    "                \n",
    "                koi_steff = st.number_input(\"Temperatura Estelar - koi_steff (K)\", \n",
    "                                        min_value=2000, max_value=15000, value=5800,\n",
    "                                        help=\"Temperatura efectiva de la estrella\")\n",
    "                \n",
    "                koi_slogg = st.number_input(\"Gravedad Estelar - koi_slogg (log g)\", \n",
    "                                        min_value=3.0, max_value=5.5, value=4.4,\n",
    "                                        help=\"Gravedad superficial estelar\")\n",
    "                \n",
    "                # ¬°ESTE ES EL CAMPO QUE FALTABA!\n",
    "                koi_srad = st.number_input(\"Radio Estelar - koi_srad (Radios Sol)\", \n",
    "                                        min_value=0.1, max_value=10.0, value=1.0,\n",
    "                                        help=\"Radio de la estrella en unidades solares\")\n",
    "            \n",
    "            submitted = st.form_submit_button(\"üöÄ Clasificar con Modelo REAL\")\n",
    "        \n",
    "        if submitted:\n",
    "            # Verificar que tenemos todas las caracter√≠sticas\n",
    "            features = (\n",
    "                koi_period, koi_duration, koi_depth, koi_prad,\n",
    "                koi_teq, koi_insol, koi_steff, koi_slogg, koi_srad  # ¬°Ahora son 9!\n",
    "            )\n",
    "            \n",
    "            st.info(f\"üîç Enviando {len(features)} caracter√≠sticas al modelo\")\n",
    "            self._real_prediction(*features)\n",
    "\n",
    "    def _real_prediction(self, *features):\n",
    "        \"\"\"Predicci√≥n REAL con el modelo entrenado - VERSI√ìN CORREGIDA\"\"\"\n",
    "        # Cargar informaci√≥n del modelo\n",
    "        processor_path = os.path.join(PROJECT_ROOT, 'models', 'data_processor.pkl')\n",
    "        features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "        \n",
    "        try:\n",
    "            # Verificar que tenemos los archivos necesarios\n",
    "            if not os.path.exists(processor_path) or not os.path.exists(features_path):\n",
    "                st.error(\"‚ùå No se encontraron los archivos del modelo entrenado\")\n",
    "                return\n",
    "            \n",
    "            # Cargar feature names y preprocesador\n",
    "            saved_feature_names = joblib.load(features_path)\n",
    "            data_processor = joblib.load(processor_path)\n",
    "            \n",
    "            # VERIFICACI√ìN CR√çTICA: ¬øCoincide el n√∫mero de caracter√≠sticas?\n",
    "            if len(features) != len(saved_feature_names):\n",
    "                st.error(f\"\"\"\n",
    "                ‚ùå **ERROR CR√çTICO - Discrepancia en caracter√≠sticas**\n",
    "                \n",
    "                **Env√≠as:** {len(features)} caracter√≠sticas\n",
    "                **Modelo espera:** {len(saved_feature_names)} caracter√≠sticas\n",
    "                \n",
    "                **Caracter√≠sticas esperadas por el modelo:**\n",
    "                {saved_feature_names}\n",
    "                \n",
    "                **Soluci√≥n:** Aseg√∫rate de que el formulario tenga exactamente {len(saved_feature_names)} campos.\n",
    "                \"\"\")\n",
    "                return\n",
    "            \n",
    "            st.success(f\"‚úÖ Coincidencia perfecta: {len(features)} caracter√≠sticas enviadas\")\n",
    "            \n",
    "            # Crear array de caracter√≠sticas\n",
    "            feature_array = np.array([features]).reshape(1, -1)\n",
    "            \n",
    "            # Escalar caracter√≠sticas\n",
    "            feature_array_scaled = data_processor.scaler.transform(feature_array)\n",
    "            \n",
    "            # Realizar predicci√≥n\n",
    "            prediction = self.model.model.predict(feature_array_scaled)[0]\n",
    "            probability = self.model.model.predict_proba(feature_array_scaled)[0, 1]\n",
    "            \n",
    "            # Mostrar resultados\n",
    "            st.subheader(\"üéØ Resultado de la Clasificaci√≥n REAL\")\n",
    "            \n",
    "            col1, col2 = st.columns([1, 2])\n",
    "            \n",
    "            with col1:\n",
    "                if prediction == 1:\n",
    "                    st.success(\"‚úÖ **EXOPLANETA DETECTADO**\")\n",
    "                    st.balloons()\n",
    "                else:\n",
    "                    st.error(\"‚ùå **NO ES EXOPLANETA**\")\n",
    "                \n",
    "                st.metric(\"Probabilidad\", f\"{probability:.2%}\")\n",
    "                \n",
    "                # Interpretaci√≥n de la probabilidad\n",
    "                if probability >= 0.8:\n",
    "                    st.info(\"üü¢ **Alta confianza** - Muy probable exoplaneta\")\n",
    "                elif probability >= 0.6:\n",
    "                    st.info(\"üü° **Confianza media** - Posible exoplaneta\")\n",
    "                else:\n",
    "                    st.info(\"üî¥ **Baja confianza** - Probable falso positivo\")\n",
    "            \n",
    "            with col2:\n",
    "                # An√°lisis detallado de caracter√≠sticas\n",
    "                st.markdown(\"#### üìä An√°lisis de Caracter√≠sticas\")\n",
    "                \n",
    "                # Mapeo de nombres amigables\n",
    "                feature_display_names = {\n",
    "                    'koi_period': 'Per√≠odo Orbital',\n",
    "                    'koi_duration': 'Duraci√≥n Tr√°nsito', \n",
    "                    'koi_depth': 'Profundidad Tr√°nsito',\n",
    "                    'koi_prad': 'Radio Planetario',\n",
    "                    'koi_teq': 'Temperatura Planeta',\n",
    "                    'koi_insol': 'Flujo Insolaci√≥n',\n",
    "                    'koi_steff': 'Temperatura Estelar',\n",
    "                    'koi_slogg': 'Gravedad Estelar',\n",
    "                    'koi_srad': 'Radio Estelar'\n",
    "                }\n",
    "                \n",
    "                # Mapeo de unidades\n",
    "                feature_units = {\n",
    "                    'koi_period': 'd√≠as',\n",
    "                    'koi_duration': 'horas', \n",
    "                    'koi_depth': 'ppm',\n",
    "                    'koi_prad': 'R‚äï',\n",
    "                    'koi_teq': 'K',\n",
    "                    'koi_insol': 'S‚äï',\n",
    "                    'koi_steff': 'K',\n",
    "                    'koi_slogg': 'log g',\n",
    "                    'koi_srad': 'R‚òâ'\n",
    "                }\n",
    "                \n",
    "                # Crear tabla de an√°lisis\n",
    "                analysis_data = []\n",
    "                for i, feature_name in enumerate(saved_feature_names):\n",
    "                    display_name = feature_display_names.get(feature_name, feature_name)\n",
    "                    units = feature_units.get(feature_name, '')\n",
    "                    value = features[i]\n",
    "                    \n",
    "                    analysis_data.append({\n",
    "                        'Caracter√≠stica': display_name,\n",
    "                        'Valor': f\"{value} {units}\",\n",
    "                        'C√≥digo': feature_name\n",
    "                    })\n",
    "                \n",
    "                analysis_df = pd.DataFrame(analysis_data)\n",
    "                st.dataframe(analysis_df, use_container_width=True, hide_index=True)\n",
    "                \n",
    "                # Informaci√≥n adicional\n",
    "                st.markdown(\"#### üí° Informaci√≥n del Modelo\")\n",
    "                st.info(f\"\"\"\n",
    "                - **Modelo:** Ensemble Stacking (4 algoritmos)\n",
    "                - **Caracter√≠sticas:** {len(saved_feature_names)}\n",
    "                - **Precisi√≥n:** ~83%\n",
    "                - **Datos de entrenamiento:** Kepler + K2 + TESS (NASA)\n",
    "                \"\"\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error en la predicci√≥n: {e}\")\n",
    "            st.info(\"üí° **Soluci√≥n:** Reentrena el modelo en la pesta√±a 'Entrenar Modelo REAL'\")\n",
    "\n",
    "    def render_real_analysis(self):\n",
    "        \"\"\"An√°lisis de datos REALES\"\"\"\n",
    "        st.title(\"üìä An√°lisis de Datos REALES NASA\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar datos reales\n",
    "            kepler_df, k2_df, tess_df = self.data_processor.load_real_data()\n",
    "            \n",
    "            if kepler_df is None:\n",
    "                st.warning(\"No se pudieron cargar los datasets para an√°lisis\")\n",
    "                return\n",
    "            \n",
    "            st.success(f\"‚úÖ Datasets cargados: Kepler ({len(kepler_df):,}), K2 ({len(k2_df):,}), TESS ({len(tess_df):,})\")\n",
    "            \n",
    "            # Selector de dataset\n",
    "            dataset_choice = st.selectbox(\"Seleccionar Dataset para An√°lisis:\", \n",
    "                                        [\"Kepler\", \"K2\", \"TESS\"])\n",
    "            \n",
    "            if dataset_choice == \"Kepler\":\n",
    "                df = kepler_df\n",
    "                st.subheader(\"üî≠ Dataset Kepler\")\n",
    "            elif dataset_choice == \"K2\":\n",
    "                df = k2_df\n",
    "                st.subheader(\"üõ∞Ô∏è Dataset K2\")\n",
    "            else:\n",
    "                df = tess_df\n",
    "                st.subheader(\"üì° Dataset TESS\")\n",
    "            \n",
    "            # Mostrar informaci√≥n b√°sica\n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            \n",
    "            with col1:\n",
    "                st.metric(\"Total Registros\", f\"{len(df):,}\")\n",
    "            with col2:\n",
    "                st.metric(\"Columnas\", df.shape[1])\n",
    "            with col3:\n",
    "                missing = df.isnull().sum().sum()\n",
    "                st.metric(\"Valores Missing\", f\"{missing:,}\")\n",
    "            \n",
    "            # Vista previa de datos\n",
    "            st.subheader(\"üëÄ Vista Previa de Datos\")\n",
    "            st.dataframe(df.head(10), use_container_width=True)\n",
    "            \n",
    "            # An√°lisis de columnas\n",
    "            st.subheader(\"üìã Columnas Disponibles\")\n",
    "            st.write(f\"Total de columnas: {len(df.columns)}\")\n",
    "            st.write(list(df.columns))\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error en el an√°lisis: {e}\")\n",
    "\n",
    "    def render_saved_models(self):\n",
    "        \"\"\"Gesti√≥n de modelos guardados - VERSI√ìN MEJORADA\"\"\"\n",
    "        st.title(\"üíæ Modelos Guardados\")\n",
    "        \n",
    "        models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "        \n",
    "        # Crear la carpeta si no existe\n",
    "        if not os.path.exists(models_dir):\n",
    "            st.warning(\"üìÅ La carpeta de modelos no existe. Cre√°ndola...\")\n",
    "            os.makedirs(models_dir, exist_ok=True)\n",
    "            st.success(f\"‚úÖ Carpeta creada: {models_dir}\")\n",
    "        \n",
    "        # Verificar archivos en la carpeta\n",
    "        try:\n",
    "            model_files = [f for f in os.listdir(models_dir) if f.endswith('.pkl')]\n",
    "        except FileNotFoundError:\n",
    "            model_files = []\n",
    "        \n",
    "        if not model_files:\n",
    "            st.info(\"\"\"\n",
    "            üì≠ **No hay modelos guardados**\n",
    "            \n",
    "            Los modelos aparecer√°n aqu√≠ despu√©s de:\n",
    "            1. üöÄ Entrenar un modelo en la pesta√±a \"Entrenar Modelo REAL\"\n",
    "            2. üíæ El modelo se guardar√° autom√°ticamente en la carpeta `models/`\n",
    "            \n",
    "            **Archivos que se guardan:**\n",
    "            - `real_ensemble_model.pkl` - Modelo ensemble principal\n",
    "            - `data_processor.pkl` - Preprocesador de datos  \n",
    "            - `feature_names.pkl` - Nombres de caracter√≠sticas\n",
    "            \"\"\")\n",
    "            \n",
    "            # Mostrar estructura esperada\n",
    "            st.subheader(\"üìÅ Estructura esperada:\")\n",
    "            st.code(\"\"\"\n",
    "            exoplanet-ai-detector/\n",
    "            ‚îú‚îÄ‚îÄ models/\n",
    "            ‚îÇ   ‚îú‚îÄ‚îÄ real_ensemble_model.pkl\n",
    "            ‚îÇ   ‚îú‚îÄ‚îÄ data_processor.pkl  \n",
    "            ‚îÇ   ‚îî‚îÄ‚îÄ feature_names.pkl\n",
    "            ‚îú‚îÄ‚îÄ data/\n",
    "            ‚îÇ   ‚îî‚îÄ‚îÄ raw/\n",
    "            ‚îÇ       ‚îú‚îÄ‚îÄ kepler.csv\n",
    "            ‚îÇ       ‚îú‚îÄ‚îÄ k2.csv\n",
    "            ‚îÇ       ‚îî‚îÄ‚îÄ tess.csv\n",
    "            ‚îî‚îÄ‚îÄ webapp/\n",
    "                ‚îî‚îÄ‚îÄ app.py\n",
    "            \"\"\")\n",
    "            return\n",
    "        \n",
    "        st.success(f\"‚úÖ Se encontraron {len(model_files)} modelos guardados\")\n",
    "        \n",
    "        # Mostrar modelos disponibles\n",
    "        st.subheader(\"üìÅ Modelos Disponibles\")\n",
    "        \n",
    "        for model_file in model_files:\n",
    "            file_path = os.path.join(models_dir, model_file)\n",
    "            file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "            file_time = os.path.getmtime(file_path)\n",
    "            from datetime import datetime\n",
    "            file_date = datetime.fromtimestamp(file_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            col1, col2, col3, col4, col5 = st.columns([3, 1, 1, 1, 1])\n",
    "            \n",
    "            with col1:\n",
    "                # Icono diferente seg√∫n el tipo de archivo\n",
    "                if \"ensemble\" in model_file:\n",
    "                    icon = \"ü§ñ\"\n",
    "                elif \"processor\" in model_file:\n",
    "                    icon = \"üîß\" \n",
    "                elif \"feature\" in model_file:\n",
    "                    icon = \"üìä\"\n",
    "                else:\n",
    "                    icon = \"üìÑ\"\n",
    "                    \n",
    "                st.write(f\"{icon} **{model_file}**\")\n",
    "                st.caption(f\"Creado: {file_date}\")\n",
    "                \n",
    "            with col2:\n",
    "                st.write(f\"{file_size:.1f} MB\")\n",
    "                \n",
    "            with col3:\n",
    "                # Bot√≥n de informaci√≥n\n",
    "                if st.button(\"‚ÑπÔ∏è\", key=f\"info_{model_file}\", help=\"Ver informaci√≥n\"):\n",
    "                    self._show_model_info(model_file, file_path)\n",
    "                    \n",
    "            with col4:\n",
    "                # Bot√≥n de carga\n",
    "                if st.button(\"üì•\", key=f\"load_{model_file}\", help=\"Cargar modelo\"):\n",
    "                    if self._load_specific_model(model_file):\n",
    "                        st.success(f\"‚úÖ {model_file} cargado\")\n",
    "                        self.model_trained = True\n",
    "                    else:\n",
    "                        st.error(f\"‚ùå Error cargando {model_file}\")\n",
    "                        \n",
    "            with col5:\n",
    "                # Bot√≥n de eliminaci√≥n con confirmaci√≥n\n",
    "                if st.button(\"üóëÔ∏è\", key=f\"delete_{model_file}\", help=\"Eliminar modelo\"):\n",
    "                    if st.checkbox(f\"¬øConfirmar eliminaci√≥n de {model_file}?\", key=f\"confirm_{model_file}\"):\n",
    "                        try:\n",
    "                            os.remove(file_path)\n",
    "                            st.success(f\"‚úÖ {model_file} eliminado\")\n",
    "                            st.rerun()\n",
    "                        except Exception as e:\n",
    "                            st.error(f\"‚ùå Error eliminando: {e}\")\n",
    "        \n",
    "        # Estad√≠sticas de la carpeta\n",
    "        st.subheader(\"üìà Estad√≠sticas de Modelos\")\n",
    "        total_size = sum(os.path.getsize(os.path.join(models_dir, f)) for f in model_files) / 1024 / 1024\n",
    "        \n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        with col1:\n",
    "            st.metric(\"Total Modelos\", len(model_files))\n",
    "        with col2:\n",
    "            st.metric(\"Espacio Total\", f\"{total_size:.1f} MB\")\n",
    "        with col3:\n",
    "            st.metric(\"Carpeta\", \"models/\")\n",
    "        \n",
    "        # Informaci√≥n de uso\n",
    "        st.subheader(\"‚ÑπÔ∏è Informaci√≥n de Archivos\")\n",
    "        st.info(\"\"\"\n",
    "        **Archivos del sistema:**\n",
    "        - ü§ñ **real_ensemble_model.pkl** - Modelo ensemble principal (Random Forest + XGBoost + LightGBM + Extra Trees)\n",
    "        - üîß **data_processor.pkl** - Preprocesador con scaler y configuraci√≥n de caracter√≠sticas\n",
    "        - üìä **feature_names.pkl** - Nombres y orden de las caracter√≠sticas usadas en el entrenamiento\n",
    "        \n",
    "        **Recomendaciones:**\n",
    "        - No elimines archivos manualmente desde el sistema de archivos\n",
    "        - Usa los botones de esta interfaz para gesti√≥n segura\n",
    "        - Los 3 archivos deben estar presentes para que el sistema funcione correctamente\n",
    "        \"\"\")\n",
    "        \n",
    "        # Bot√≥n para crear modelo de ejemplo (para testing)\n",
    "        st.subheader(\"üõ†Ô∏è Herramientas\")\n",
    "        if st.button(\"üß™ Crear Modelo de Ejemplo\", help=\"Crear un modelo dummy para testing\"):\n",
    "            self._create_example_model()\n",
    "            \n",
    "        if st.button(\"üîÑ Actualizar Lista\", help=\"Refrescar la lista de modelos\"):\n",
    "            st.rerun()\n",
    "\n",
    "    def _show_model_info(self, model_file, file_path):\n",
    "        \"\"\"Mostrar informaci√≥n detallada de un modelo\"\"\"\n",
    "        try:\n",
    "            if \"ensemble\" in model_file:\n",
    "                model = joblib.load(file_path)\n",
    "                st.info(f\"\"\"\n",
    "                **ü§ñ Modelo Ensemble: {model_file}**\n",
    "                \n",
    "                - **Tipo:** StackingClassifier\n",
    "                - **Algoritmos base:** {len(model.named_estimators_)}\n",
    "                - **Estimadores:** {list(model.named_estimators_.keys())}\n",
    "                - **Meta-estimador:** {type(model.final_estimator_).__name__}\n",
    "                \"\"\")\n",
    "                \n",
    "            elif \"processor\" in model_file:\n",
    "                processor = joblib.load(file_path)\n",
    "                st.info(f\"\"\"\n",
    "                **üîß Preprocesador: {model_file}**\n",
    "                \n",
    "                - **Tipo:** ExoplanetDataProcessor\n",
    "                - **Caracter√≠sticas escaladas:** {len(processor.feature_names) if hasattr(processor, 'feature_names') else 'N/A'}\n",
    "                - **Scaler:** {type(processor.scaler).__name__ if hasattr(processor, 'scaler') else 'N/A'}\n",
    "                \"\"\")\n",
    "                \n",
    "            elif \"feature\" in model_file:\n",
    "                features = joblib.load(file_path)\n",
    "                st.info(f\"\"\"\n",
    "                **üìä Caracter√≠sticas: {model_file}**\n",
    "                \n",
    "                - **N√∫mero de caracter√≠sticas:** {len(features)}\n",
    "                - **Caracter√≠sticas:** {features}\n",
    "                \"\"\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error cargando informaci√≥n de {model_file}: {e}\")\n",
    "\n",
    "    def _load_specific_model(self, model_file):\n",
    "        \"\"\"Cargar un modelo espec√≠fico\"\"\"\n",
    "        try:\n",
    "            model_path = os.path.join(PROJECT_ROOT, 'models', model_file)\n",
    "            \n",
    "            if \"ensemble\" in model_file:\n",
    "                return self.model.load_model(model_path)\n",
    "            else:\n",
    "                st.info(f\"üì• {model_file} cargado (no es el modelo principal)\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error cargando {model_file}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _create_example_model(self):\n",
    "        \"\"\"Crear un modelo de ejemplo para testing\"\"\"\n",
    "        try:\n",
    "            models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "            os.makedirs(models_dir, exist_ok=True)\n",
    "            \n",
    "            # Crear feature names de ejemplo\n",
    "            feature_names = ['koi_period', 'koi_duration', 'koi_depth', 'koi_prad', \n",
    "                            'koi_teq', 'koi_insol', 'koi_steff', 'koi_slogg', 'koi_srad']\n",
    "            joblib.dump(feature_names, os.path.join(models_dir, 'feature_names.pkl'))\n",
    "            \n",
    "            # Crear processor de ejemplo\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            class ExampleProcessor:\n",
    "                def __init__(self):\n",
    "                    self.scaler = StandardScaler()\n",
    "                    self.feature_names = feature_names\n",
    "            processor = ExampleProcessor()\n",
    "            joblib.dump(processor, os.path.join(models_dir, 'data_processor.pkl'))\n",
    "            \n",
    "            st.success(\"‚úÖ Modelo de ejemplo creado para testing\")\n",
    "            st.rerun()\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error creando modelo de ejemplo: {e}\")\n",
    "\n",
    "    def render_batch_classification(self):\n",
    "        \"\"\"Clasificaci√≥n por lotes de archivos CSV completos\"\"\"\n",
    "        st.title(\"üì¶ Clasificaci√≥n por Lotes\")\n",
    "        \n",
    "        st.markdown(\"\"\"\n",
    "        ### üöÄ Clasificaci√≥n Masiva de Exoplanetas\n",
    "        \n",
    "        **Sube un archivo CSV completo** (como Kepler, K2 o TESS) y el sistema:\n",
    "        - ‚úÖ Clasificar√° autom√°ticamente todos los candidatos\n",
    "        - üìä Mostrar√° estad√≠sticas completas\n",
    "        - üîç Identificar√° exoplanetas detectados\n",
    "        - üíæ Permitir√° descargar resultados\n",
    "        \n",
    "        **Formatos compatibles:** Kepler, K2, TESS o cualquier CSV con las 9 caracter√≠sticas requeridas\n",
    "        \"\"\")\n",
    "        \n",
    "        # Verificar si hay modelo entrenado\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        if not os.path.exists(model_path):\n",
    "            st.error(\"\"\"\n",
    "            ‚ùå **No hay modelo entrenado**\n",
    "            \n",
    "            Para usar la clasificaci√≥n por lotes:\n",
    "            1. Ve a la pesta√±a **'Entrenar Modelo REAL'**\n",
    "            2. Entrena el modelo con tus datos de la NASA\n",
    "            3. Regresa aqu√≠ para clasificar archivos completos\n",
    "            \"\"\")\n",
    "            return\n",
    "        \n",
    "        # Cargar modelo si no est√° cargado\n",
    "        if not self.model_trained:\n",
    "            if self.model.load_model(model_path):\n",
    "                self.model_trained = True\n",
    "                st.success(\"‚úÖ Modelo REAL cargado exitosamente\")\n",
    "            else:\n",
    "                st.error(\"‚ùå Error cargando el modelo\")\n",
    "                return\n",
    "        \n",
    "        # Secci√≥n de carga de archivos\n",
    "        st.subheader(\"üì§ Cargar Archivo CSV\")\n",
    "        \n",
    "        uploaded_file = st.file_uploader(\n",
    "            \"Selecciona un archivo CSV para clasificar\", \n",
    "            type=['csv'],\n",
    "            help=\"Archivos compatibles: Kepler, K2, TESS o cualquier CSV con las caracter√≠sticas requeridas\"\n",
    "        )\n",
    "        \n",
    "        if uploaded_file is not None:\n",
    "            try:\n",
    "                # Leer el archivo\n",
    "                df = pd.read_csv(uploaded_file)\n",
    "                st.success(f\"‚úÖ Archivo cargado: {uploaded_file.name}\")\n",
    "                st.info(f\"üìä Datos: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "                \n",
    "                # Mostrar vista previa\n",
    "                with st.expander(\"üëÄ Vista previa del archivo cargado\"):\n",
    "                    st.dataframe(df.head(10), use_container_width=True)\n",
    "                    st.write(f\"**Columnas disponibles:** {list(df.columns)}\")\n",
    "                \n",
    "                # Procesar el archivo\n",
    "                if st.button(\"üéØ Ejecutar Clasificaci√≥n Masiva\", type=\"primary\"):\n",
    "                    with st.spinner(\"üîç Clasificando candidatos... Esto puede tomar unos segundos\"):\n",
    "                        results = self._batch_predict(df, uploaded_file.name)\n",
    "                        \n",
    "                        if results is not None:\n",
    "                            self._display_batch_results(results, df)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                st.error(f\"‚ùå Error procesando el archivo: {e}\")\n",
    "\n",
    "    def _batch_predict(self, df, filename):\n",
    "        \"\"\"Realizar predicciones por lotes\"\"\"\n",
    "        try:\n",
    "            # Cargar preprocesador y feature names\n",
    "            processor_path = os.path.join(PROJECT_ROOT, 'models', 'data_processor.pkl')\n",
    "            features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "            \n",
    "            if not os.path.exists(processor_path) or not os.path.exists(features_path):\n",
    "                st.error(\"‚ùå No se encontraron los archivos del modelo entrenado\")\n",
    "                return None\n",
    "            \n",
    "            saved_feature_names = joblib.load(features_path)\n",
    "            data_processor = joblib.load(processor_path)\n",
    "            \n",
    "            st.info(f\"üîç Modelo espera {len(saved_feature_names)} caracter√≠sticas: {saved_feature_names}\")\n",
    "            \n",
    "            # Verificar que tenemos las caracter√≠sticas necesarias\n",
    "            missing_features = [f for f in saved_feature_names if f not in df.columns]\n",
    "            if missing_features:\n",
    "                st.error(f\"‚ùå Faltan caracter√≠sticas en el archivo: {missing_features}\")\n",
    "                st.info(\"üí° **Soluci√≥n:** Aseg√∫rate de que el CSV tenga las mismas columnas que los datos de entrenamiento\")\n",
    "                return None\n",
    "            \n",
    "            # Seleccionar y preparar caracter√≠sticas\n",
    "            X = df[saved_feature_names].copy()\n",
    "            \n",
    "            # Manejar valores missing\n",
    "            missing_before = X.isnull().sum().sum()\n",
    "            if missing_before > 0:\n",
    "                st.warning(f\"‚ö†Ô∏è Se encontraron {missing_before} valores missing. Imputando con medianas...\")\n",
    "                X = X.fillna(X.median())\n",
    "            \n",
    "            # Escalar caracter√≠sticas\n",
    "            X_scaled = data_processor.scaler.transform(X)\n",
    "            \n",
    "            # Realizar predicciones\n",
    "            predictions = self.model.model.predict(X_scaled)\n",
    "            probabilities = self.model.model.predict_proba(X_scaled)[:, 1]\n",
    "            \n",
    "            # Crear DataFrame de resultados\n",
    "            results_df = df.copy()\n",
    "            results_df['prediction'] = predictions\n",
    "            results_df['probability'] = probabilities\n",
    "            results_df['classification'] = results_df['prediction'].map({1: 'EXOPLANETA', 0: 'NO_EXOPLANETA'})\n",
    "            \n",
    "            # A√±adir confianza\n",
    "            results_df['confidence'] = results_df['probability'].apply(\n",
    "                lambda x: 'ALTA' if x >= 0.8 else 'MEDIA' if x >= 0.6 else 'BAJA'\n",
    "            )\n",
    "            \n",
    "            return results_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error en clasificaci√≥n por lotes: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _display_batch_results(self, results_df, original_df):\n",
    "        \"\"\"Mostrar resultados de clasificaci√≥n por lotes\"\"\"\n",
    "        st.success(\"‚úÖ Clasificaci√≥n completada exitosamente!\")\n",
    "        \n",
    "        # Estad√≠sticas generales\n",
    "        total_candidates = len(results_df)\n",
    "        exoplanets_detected = results_df['prediction'].sum()\n",
    "        non_exoplanets = total_candidates - exoplanets_detected\n",
    "        \n",
    "        st.subheader(\"üìà Resumen de Clasificaci√≥n\")\n",
    "        \n",
    "        # M√©tricas principales\n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        \n",
    "        with col1:\n",
    "            st.metric(\"Total Candidatos\", total_candidates)\n",
    "        with col2:\n",
    "            st.metric(\"Exoplanetas Detectados\", exoplanets_detected)\n",
    "        with col3:\n",
    "            st.metric(\"No Exoplanetas\", non_exoplanets)\n",
    "        with col4:\n",
    "            detection_rate = (exoplanets_detected / total_candidates) * 100\n",
    "            st.metric(\"Tasa de Detecci√≥n\", f\"{detection_rate:.1f}%\")\n",
    "        \n",
    "        # Distribuci√≥n de confianza\n",
    "        confidence_counts = results_df['confidence'].value_counts()\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            # Gr√°fico de distribuci√≥n\n",
    "            fig = px.pie(\n",
    "                results_df, \n",
    "                names='classification',\n",
    "                title='Distribuci√≥n: Exoplanetas vs No Exoplanetas',\n",
    "                color='classification',\n",
    "                color_discrete_map={'EXOPLANETA': '#00CC96', 'NO_EXOPLANETA': '#EF553B'}\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        with col2:\n",
    "            # Gr√°fico de confianza\n",
    "            fig = px.bar(\n",
    "                confidence_counts,\n",
    "                title='Distribuci√≥n por Nivel de Confianza',\n",
    "                labels={'index': 'Confianza', 'value': 'Cantidad'},\n",
    "                color=confidence_counts.index,\n",
    "                color_discrete_map={'ALTA': '#00CC96', 'MEDIA': '#FECB52', 'BAJA': '#EF553B'}\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        # Mostrar exoplanetas detectados\n",
    "        st.subheader(\"üîç Exoplanetas Detectados\")\n",
    "        \n",
    "        exoplanets_df = results_df[results_df['prediction'] == 1].copy()\n",
    "        \n",
    "        if len(exoplanets_df) > 0:\n",
    "            st.success(f\"üéØ Se encontraron {len(exoplanets_df)} exoplanetas potenciales\")\n",
    "            \n",
    "            # Ordenar por probabilidad (mayor a menor)\n",
    "            exoplanets_df = exoplanets_df.sort_values('probability', ascending=False)\n",
    "            \n",
    "            # Seleccionar columnas importantes para mostrar\n",
    "            display_columns = []\n",
    "            possible_columns = [\n",
    "                'kepid', 'kepoi_name', 'kepler_name', 'koi_disposition',\n",
    "                'koi_period', 'koi_prad', 'koi_teq', 'probability', 'confidence'\n",
    "            ]\n",
    "            \n",
    "            for col in possible_columns:\n",
    "                if col in exoplanets_df.columns:\n",
    "                    display_columns.append(col)\n",
    "            \n",
    "            # A√±adir columnas de resultado si no est√°n\n",
    "            if 'probability' not in display_columns:\n",
    "                display_columns.extend(['probability', 'confidence'])\n",
    "            \n",
    "            # Mostrar tabla de exoplanetas\n",
    "            st.dataframe(\n",
    "                exoplanets_df[display_columns].head(50),  # Mostrar m√°ximo 50\n",
    "                use_container_width=True,\n",
    "                height=400\n",
    "            )\n",
    "            \n",
    "            # Estad√≠sticas de los exoplanetas detectados\n",
    "            st.subheader(\"üìä Estad√≠sticas de Exoplanetas Detectados\")\n",
    "            \n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            \n",
    "            with col1:\n",
    "                avg_probability = exoplanets_df['probability'].mean()\n",
    "                st.metric(\"Probabilidad Promedio\", f\"{avg_probability:.1%}\")\n",
    "            \n",
    "            with col2:\n",
    "                high_confidence = len(exoplanets_df[exoplanets_df['confidence'] == 'ALTA'])\n",
    "                st.metric(\"Alta Confianza\", high_confidence)\n",
    "            \n",
    "            with col3:\n",
    "                if 'koi_prad' in exoplanets_df.columns:\n",
    "                    avg_radius = exoplanets_df['koi_prad'].mean()\n",
    "                    st.metric(\"Radio Promedio\", f\"{avg_radius:.1f} R‚äï\")\n",
    "            \n",
    "            # Distribuci√≥n de caracter√≠sticas importantes\n",
    "            if 'koi_prad' in exoplanets_df.columns and 'koi_period' in exoplanets_df.columns:\n",
    "                st.subheader(\"üìà Caracter√≠sticas de Exoplanetas Detectados\")\n",
    "                \n",
    "                col1, col2 = st.columns(2)\n",
    "                \n",
    "                with col1:\n",
    "                    fig = px.histogram(\n",
    "                        exoplanets_df,\n",
    "                        x='koi_prad',\n",
    "                        title='Distribuci√≥n de Radios Planetarios',\n",
    "                        labels={'koi_prad': 'Radio (Radios Tierra)'},\n",
    "                        nbins=20\n",
    "                    )\n",
    "                    st.plotly_chart(fig, use_container_width=True)\n",
    "                \n",
    "                with col2:\n",
    "                    fig = px.scatter(\n",
    "                        exoplanets_df,\n",
    "                        x='koi_period',\n",
    "                        y='probability',\n",
    "                        color='confidence',\n",
    "                        title='Per√≠odo Orbital vs Probabilidad',\n",
    "                        labels={'koi_period': 'Per√≠odo Orbital (d√≠as)', 'probability': 'Probabilidad'},\n",
    "                        color_discrete_map={'ALTA': '#00CC96', 'MEDIA': '#FECB52', 'BAJA': '#EF553B'}\n",
    "                    )\n",
    "                    st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Descargar resultados\n",
    "            st.subheader(\"üíæ Descargar Resultados\")\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                # Descargar solo exoplanetas\n",
    "                csv_exoplanets = exoplanets_df.to_csv(index=False)\n",
    "                st.download_button(\n",
    "                    label=\"üì• Descargar Exoplanetas Detectados\",\n",
    "                    data=csv_exoplanets,\n",
    "                    file_name=f\"exoplanetas_detectados_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.csv\",\n",
    "                    mime=\"text/csv\",\n",
    "                    help=\"Descargar solo los candidatos clasificados como exoplanetas\"\n",
    "                )\n",
    "            \n",
    "            with col2:\n",
    "                # Descargar todos los resultados\n",
    "                csv_all = results_df.to_csv(index=False)\n",
    "                st.download_button(\n",
    "                    label=\"üì• Descargar Todos los Resultados\",\n",
    "                    data=csv_all,\n",
    "                    file_name=f\"clasificacion_completa_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.csv\",\n",
    "                    mime=\"text/csv\",\n",
    "                    help=\"Descargar todos los candidatos con sus clasificaciones\"\n",
    "                )\n",
    "            \n",
    "            # Informaci√≥n adicional\n",
    "            st.info(\"\"\"\n",
    "            üí° **Interpretaci√≥n de resultados:**\n",
    "            - **ALTA confianza:** Probabilidad ‚â• 80% - Muy probable exoplaneta\n",
    "            - **MEDIA confianza:** Probabilidad 60-79% - Posible exoplaneta  \n",
    "            - **BAJA confianza:** Probabilidad < 60% - Requiere m√°s an√°lisis\n",
    "            \"\"\")\n",
    "            \n",
    "        else:\n",
    "            st.warning(\"‚ö†Ô∏è No se detectaron exoplanetas en este archivo\")\n",
    "            \n",
    "            # Mostrar algunos candidatos con mayor probabilidad\n",
    "            top_candidates = results_df.nlargest(5, 'probability')\n",
    "            if len(top_candidates) > 0:\n",
    "                st.subheader(\"üéØ Candidatos M√°s Prometedores\")\n",
    "                st.dataframe(\n",
    "                    top_candidates[['probability', 'confidence'] + \n",
    "                                [col for col in top_candidates.columns if col.startswith('koi_')][:5]],\n",
    "                    use_container_width=True\n",
    "                )\n",
    "\n",
    "    def _get_sample_datasets_info(self):\n",
    "        \"\"\"Informaci√≥n sobre datasets de ejemplo\"\"\"\n",
    "        st.subheader(\"üõ†Ô∏è Datasets de Prueba\")\n",
    "        \n",
    "        st.markdown(\"\"\"\n",
    "        **Puedes probar con estos datasets de ejemplo:**\n",
    "        \n",
    "        - **Kepler:** [Descargar de NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=cumulative)\n",
    "        - **K2:** [Descargar de NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=k2targets)\n",
    "        - **TESS:** [Descargar de NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=toi)\n",
    "        \n",
    "        **Estructura m√≠nima requerida:**\n",
    "        ```csv\n",
    "        koi_period,koi_duration,koi_depth,koi_prad,koi_teq,koi_insol,koi_steff,koi_slogg,koi_srad\n",
    "        10.5,3.2,1500,2.1,450,95.0,5800,4.4,1.0\n",
    "        15.3,4.1,800,1.8,320,75.5,5200,4.5,0.9\n",
    "        ```\n",
    "        \"\"\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Ejecutar la aplicaci√≥n completa - ACTUALIZADO\"\"\"\n",
    "        page = self.render_sidebar()\n",
    "        \n",
    "        if page == \"üè† Inicio\":\n",
    "            self.render_home()\n",
    "        elif page == \"üöÄ Entrenar Modelo REAL\":\n",
    "            self.render_real_training()\n",
    "        elif page == \"ü§ñ Clasificar Exoplanetas\":\n",
    "            self.render_real_classification()\n",
    "        elif page == \"üì¶ Clasificaci√≥n por Lotes\":  # ¬°NUEVA P√ÅGINA!\n",
    "            self.render_batch_classification()\n",
    "        elif page == \"üìä An√°lisis de Datos REAL\":\n",
    "            self.render_real_analysis()\n",
    "        elif page == \"üíæ Modelos Guardados\":\n",
    "            self.render_saved_models()\n",
    "\n",
    "# Ejecutar la aplicaci√≥n\n",
    "if __name__ == \"__main__\":\n",
    "    app = ExoplanetDetectorApp()\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
