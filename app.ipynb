{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a3f3c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# webapp/app.py - VERSIÃ“N CON RUTAS CORREGIDAS\n",
    "!pip install streamlit\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OBTENER LA RUTA CORRECTA DEL PROYECTO\n",
    "def get_project_root():\n",
    "    \"\"\"Obtener la ruta raÃ­z del proyecto de forma confiable\"\"\"\n",
    "    current_file = os.path.abspath(__file__)\n",
    "    project_root = os.path.dirname(os.path.dirname(current_file))\n",
    "    return project_root\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "\n",
    "class ExoplanetDataProcessor:\n",
    "    \"\"\"Procesador de datos para los datasets reales de la NASA\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def load_real_data(self):\n",
    "        \"\"\"Cargar los datasets reales de la NASA con rutas corregidas\"\"\"\n",
    "        try:\n",
    "            # Usar rutas absolutas desde la raÃ­z del proyecto\n",
    "            data_dir = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "            \n",
    "            st.info(f\"ğŸ” Buscando datos en: {data_dir}\")\n",
    "            \n",
    "            # Listar archivos en el directorio\n",
    "            if os.path.exists(data_dir):\n",
    "                files = os.listdir(data_dir)\n",
    "                st.info(f\"ğŸ“ Archivos encontrados en data/raw/: {files}\")\n",
    "            else:\n",
    "                st.error(f\"âŒ No existe el directorio: {data_dir}\")\n",
    "                return None, None, None\n",
    "            \n",
    "            # Construir rutas completas\n",
    "            kepler_path = os.path.join(data_dir, 'kepler.csv')\n",
    "            k2_path = os.path.join(data_dir, 'k2.csv') \n",
    "            tess_path = os.path.join(data_dir, 'tess.csv')\n",
    "            \n",
    "            st.info(f\"ğŸ“Š Intentando cargar:\\n- {kepler_path}\\n- {k2_path}\\n- {tess_path}\")\n",
    "            \n",
    "            # Verificar que los archivos existen\n",
    "            if not os.path.exists(kepler_path):\n",
    "                st.error(f\"âŒ No existe: {kepler_path}\")\n",
    "                # Buscar archivos similares\n",
    "                csv_files = [f for f in files if f.endswith('.csv')]\n",
    "                if csv_files:\n",
    "                    st.info(f\"ğŸ“„ Archivos CSV disponibles: {csv_files}\")\n",
    "                return None, None, None\n",
    "            \n",
    "            # Cargar los archivos reales\n",
    "            kepler_df = pd.read_csv(kepler_path)\n",
    "            k2_df = pd.read_csv(k2_path) if os.path.exists(k2_path) else None\n",
    "            tess_df = pd.read_csv(tess_path) if os.path.exists(tess_path) else None\n",
    "            \n",
    "            st.success(f\"âœ… Kepler cargado: {len(kepler_df)} registros\")\n",
    "            if k2_df is not None:\n",
    "                st.success(f\"âœ… K2 cargado: {len(k2_df)} registros\")\n",
    "            if tess_df is not None:\n",
    "                st.success(f\"âœ… TESS cargado: {len(tess_df)} registros\")\n",
    "            \n",
    "            return kepler_df, k2_df, tess_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"âŒ Error cargando datasets: {e}\")\n",
    "            return None, None, None\n",
    "    \n",
    "    def preprocess_kepler(self, df):\n",
    "        \"\"\"Preprocesar datos Kepler reales - VERSIÃ“N MEJORADA\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        st.info(\"ğŸ”§ Procesando datos Kepler...\")\n",
    "        \n",
    "        # Mostrar columnas disponibles\n",
    "        st.write(f\"ğŸ“‹ Columnas en Kepler: {list(df_clean.columns)}\")\n",
    "        \n",
    "        # Verificar si existe la columna de target\n",
    "        if 'koi_disposition' not in df_clean.columns:\n",
    "            st.error(\"âŒ No se encuentra la columna 'koi_disposition' en Kepler\")\n",
    "            st.info(\"Las columnas disponibles son:\")\n",
    "            st.write(list(df_clean.columns))\n",
    "            return df_clean\n",
    "        \n",
    "        # Eliminar columnas no Ãºtiles (basado en el paper)\n",
    "        columns_to_drop = ['kepid', 'kepoi_name', 'kepler_name', 'koi_pdisposition', 'koi_score']\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df_clean.columns]\n",
    "        \n",
    "        if columns_to_drop:\n",
    "            df_clean = df_clean.drop(columns=columns_to_drop)\n",
    "            st.write(f\"ğŸ—‘ï¸ Columnas eliminadas: {columns_to_drop}\")\n",
    "        \n",
    "        # Mostrar valores Ãºnicos en la columna de disposiciÃ³n\n",
    "        st.write(f\"ğŸ¯ Valores en koi_disposition: {df_clean['koi_disposition'].unique()}\")\n",
    "        \n",
    "        # Filtrar solo confirmed, candidate y false positive\n",
    "        valid_dispositions = ['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE']\n",
    "        mask = df_clean['koi_disposition'].isin(valid_dispositions)\n",
    "        df_clean = df_clean[mask]\n",
    "        \n",
    "        st.write(f\"ğŸ“Š DistribuciÃ³n despuÃ©s de filtrar: {df_clean['koi_disposition'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Crear target binario\n",
    "        df_clean['target'] = df_clean['koi_disposition'].map({\n",
    "            'CONFIRMED': 1, \n",
    "            'CANDIDATE': 1,\n",
    "            'FALSE POSITIVE': 0\n",
    "        })\n",
    "        \n",
    "        # AÃ±adir identificador de misiÃ³n\n",
    "        df_clean['mission'] = 'kepler'\n",
    "        \n",
    "        st.success(f\"âœ… Kepler procesado: {len(df_clean)} registros\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def preprocess_k2(self, df):\n",
    "        \"\"\"Preprocesar datos K2 reales\"\"\"\n",
    "        if df is None:\n",
    "            st.warning(\"âš ï¸ Dataset K2 no disponible\")\n",
    "            return None\n",
    "            \n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        st.info(\"ğŸ”§ Procesando datos K2...\")\n",
    "        st.write(f\"ğŸ“‹ Columnas en K2: {list(df_clean.columns)}\")\n",
    "        \n",
    "        # Verificar columnas necesarias\n",
    "        if 'disposition' not in df_clean.columns:\n",
    "            st.error(\"âŒ No se encuentra la columna 'disposition' en K2\")\n",
    "            return None\n",
    "        \n",
    "        # Filtrar solo confirmed y candidate\n",
    "        df_clean = df_clean[df_clean['disposition'].isin(['CONFIRMED', 'CANDIDATE'])]\n",
    "        \n",
    "        # Target binario\n",
    "        df_clean['target'] = df_clean['disposition'].map({\n",
    "            'CONFIRMED': 1,\n",
    "            'CANDIDATE': 1\n",
    "        })\n",
    "        \n",
    "        # Identificador de misiÃ³n\n",
    "        df_clean['mission'] = 'k2'\n",
    "        \n",
    "        st.success(f\"âœ… K2 procesado: {len(df_clean)} registros\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def preprocess_tess(self, df):\n",
    "        \"\"\"Preprocesar datos TESS reales\"\"\"\n",
    "        if df is None:\n",
    "            st.warning(\"âš ï¸ Dataset TESS no disponible\")\n",
    "            return None\n",
    "            \n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        st.info(\"ğŸ”§ Procesando datos TESS...\")\n",
    "        st.write(f\"ğŸ“‹ Columnas en TESS: {list(df_clean.columns)}\")\n",
    "        \n",
    "        # Verificar columnas necesarias\n",
    "        if 'tfopwg_disp' not in df_clean.columns:\n",
    "            st.error(\"âŒ No se encuentra la columna 'tfopwg_disp' en TESS\")\n",
    "            return None\n",
    "        \n",
    "        # Mapear disposiciones de TESS\n",
    "        disposition_mapping = {\n",
    "            'PC': 1, 'KP': 1, 'APC': 1,  # Positivos\n",
    "            'FP': 0, 'FA': 0  # Negativos\n",
    "        }\n",
    "        \n",
    "        df_clean['target'] = df_clean['tfopwg_disp'].map(disposition_mapping)\n",
    "        df_clean = df_clean.dropna(subset=['target'])\n",
    "        \n",
    "        # Identificador de misiÃ³n\n",
    "        df_clean['mission'] = 'tess'\n",
    "        \n",
    "        st.success(f\"âœ… TESS procesado: {len(df_clean)} registros\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Preparar caracterÃ­sticas para el modelo - VERSIÃ“N FLEXIBLE\"\"\"\n",
    "        if df is None or len(df) == 0:\n",
    "            st.error(\"âŒ No hay datos para preparar caracterÃ­sticas\")\n",
    "            return None, None, None\n",
    "            \n",
    "        st.info(\"ğŸ”§ Preparando caracterÃ­sticas...\")\n",
    "        \n",
    "        # Posibles nombres de columnas en diferentes datasets\n",
    "        possible_features = {\n",
    "            'orbital_period': ['koi_period', 'pl_orbper', 'period'],\n",
    "            'transit_duration': ['koi_duration', 'pl_trandurh', 'duration'],\n",
    "            'transit_depth': ['koi_depth', 'pl_trandep', 'depth'], \n",
    "            'planet_radius': ['koi_prad', 'pl_rade', 'radius'],\n",
    "            'equilibrium_temp': ['koi_teq', 'pl_eqt', 'teq'],\n",
    "            'insolation_flux': ['koi_insol', 'pl_insol', 'insol'],\n",
    "            'stellar_teff': ['koi_steff', 'st_teff', 'teff'],\n",
    "            'stellar_logg': ['koi_slogg', 'st_logg', 'logg'],\n",
    "            'stellar_radius': ['koi_srad', 'st_rad', 'srad']\n",
    "        }\n",
    "        \n",
    "        # Encontrar las columnas disponibles\n",
    "        available_columns = []\n",
    "        for feature_name, possible_names in possible_features.items():\n",
    "            for name in possible_names:\n",
    "                if name in df.columns:\n",
    "                    available_columns.append(name)\n",
    "                    break\n",
    "        \n",
    "        st.write(f\"ğŸ“Š Columnas numÃ©ricas encontradas: {available_columns}\")\n",
    "        \n",
    "        if not available_columns:\n",
    "            st.error(\"âŒ No se encontraron columnas numÃ©ricas para entrenar\")\n",
    "            return None, None, None\n",
    "        \n",
    "        X = df[available_columns].copy()\n",
    "        y = df['target'].values\n",
    "        \n",
    "        st.write(f\"ğŸ“Š Shape de X: {X.shape}, Shape de y: {y.shape}\")\n",
    "        \n",
    "        # Manejar valores missing\n",
    "        missing_before = X.isnull().sum().sum()\n",
    "        X = X.fillna(X.median())\n",
    "        missing_after = X.isnull().sum().sum()\n",
    "        \n",
    "        st.write(f\"ğŸ”§ Valores missing: {missing_before} antes, {missing_after} despuÃ©s\")\n",
    "        \n",
    "        # Escalar caracterÃ­sticas\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.feature_names = available_columns\n",
    "        \n",
    "        st.success(f\"âœ… CaracterÃ­sticas preparadas: {X_scaled.shape}\")\n",
    "        \n",
    "        return X_scaled, y, available_columns\n",
    "\n",
    "class RealExoplanetModel:\n",
    "    \"\"\"Modelo real para entrenamiento con datos de la NASA\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.accuracy = 0\n",
    "        self.feature_importance = None\n",
    "    \n",
    "    def create_ensemble(self):\n",
    "        \"\"\"Crear ensemble con los algoritmos del paper\"\"\"\n",
    "        base_models = [\n",
    "            ('random_forest', RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                min_samples_split=5,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('extra_trees', ExtraTreesClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('xgboost', XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                random_state=42\n",
    "            )),\n",
    "            ('lightgbm', LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        ensemble = StackingClassifier(\n",
    "            estimators=base_models,\n",
    "            final_estimator=LogisticRegression(),\n",
    "            cv=3,  # Reducido para mayor velocidad\n",
    "            passthrough=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return ensemble\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Entrenar el modelo real\"\"\"\n",
    "        if X is None or y is None:\n",
    "            st.error(\"âŒ No hay datos para entrenar\")\n",
    "            return None\n",
    "            \n",
    "        st.info(\"ğŸ¤– Iniciando entrenamiento del ensemble...\")\n",
    "        \n",
    "        self.model = self.create_ensemble()\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "        # Calcular accuracy en entrenamiento\n",
    "        y_pred = self.model.predict(X)\n",
    "        self.accuracy = accuracy_score(y, y_pred)\n",
    "        \n",
    "        st.write(f\"ğŸ“ˆ Accuracy en entrenamiento: {self.accuracy:.2%}\")\n",
    "        \n",
    "        # Calcular importancia de caracterÃ­sticas\n",
    "        self._calculate_feature_importance(X.shape[1])\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def _calculate_feature_importance(self, n_features):\n",
    "        \"\"\"Calcular importancia de caracterÃ­sticas promediada\"\"\"\n",
    "        importances = np.zeros(n_features)\n",
    "        \n",
    "        for name, model in self.model.named_estimators_.items():\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances += model.feature_importances_\n",
    "        \n",
    "        if len(self.model.named_estimators_) > 0:\n",
    "            self.feature_importance = importances / len(self.model.named_estimators_)\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Guardar modelo entrenado\"\"\"\n",
    "        if self.model:\n",
    "            # Asegurar que el directorio existe\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            joblib.dump(self.model, filepath)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Cargar modelo entrenado\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(filepath):\n",
    "                self.model = joblib.load(filepath)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error cargando modelo: {e}\")\n",
    "        return False\n",
    "\n",
    "class ExoplanetDetectorApp:\n",
    "    def __init__(self):\n",
    "        self.model = RealExoplanetModel()\n",
    "        self.data_processor = ExoplanetDataProcessor()\n",
    "        self.model_trained = False\n",
    "        \n",
    "    def render_sidebar(self):\n",
    "        \"\"\"Barra lateral de navegaciÃ³n - ACTUALIZADA\"\"\"\n",
    "        st.sidebar.title(\"ğŸ”­ NASA Exoplanet Detector - REAL\")\n",
    "        st.sidebar.markdown(\"---\")\n",
    "        \n",
    "        page = st.sidebar.radio(\"NavegaciÃ³n\", [\n",
    "            \"ğŸ  Inicio\", \n",
    "            \"ğŸš€ Entrenar Modelo REAL\",\n",
    "            \"ğŸ¤– Clasificar Exoplanetas\",\n",
    "            \"ğŸ“¦ ClasificaciÃ³n por Lotes\",  # Â¡NUEVA OPCIÃ“N!\n",
    "            \"ğŸ“Š AnÃ¡lisis de Datos REAL\",\n",
    "            \"ğŸ’¾ Modelos Guardados\"\n",
    "        ])\n",
    "        \n",
    "        st.sidebar.markdown(\"---\")\n",
    "        st.sidebar.info(\n",
    "            \"Sistema REAL con datos de Kepler, K2 y TESS de la NASA\"\n",
    "        )\n",
    "        \n",
    "        return page\n",
    "\n",
    "    def render_home(self):\n",
    "        \"\"\"PÃ¡gina de inicio\"\"\"\n",
    "        st.title(\"ğŸª NASA Exoplanet Detection AI - SISTEMA REAL\")\n",
    "        \n",
    "        col1, col2 = st.columns([2, 1])\n",
    "        \n",
    "        with col1:\n",
    "            st.markdown(\"\"\"\n",
    "            ### Sistema REAL de DetecciÃ³n de Exoplanetas\n",
    "            \n",
    "            **CaracterÃ­sticas IMPLEMENTADAS:**\n",
    "            - âœ… **Entrenamiento REAL** con datos de la NASA\n",
    "            - âœ… **Modelos PERSISTENTES** que se guardan en disco\n",
    "            - âœ… **Datos REALES** Kepler, K2 y TESS\n",
    "            - âœ… **Ensemble Stacking** como en el paper cientÃ­fico\n",
    "            - âœ… **Guardado/Auto-carga** de modelos\n",
    "            \n",
    "            **Para comenzar:**\n",
    "            1. Verifica que tus archivos CSV estÃ©n en `data/raw/`\n",
    "            2. Ve a **'Entrenar Modelo REAL'**\n",
    "            3. Â¡El sistema detectarÃ¡ automÃ¡ticamente tus datos!\n",
    "            \"\"\")\n",
    "        \n",
    "        with col2:\n",
    "            st.image(\"https://www.nasa.gov/sites/default/files/thumbnails/image/kepler_all_planets_art.jpg\", \n",
    "                    use_column_width=True,\n",
    "                    caption=\"Datos REALES de la NASA\")\n",
    "        \n",
    "        # Verificar estructura de archivos\n",
    "        st.subheader(\"ğŸ” VerificaciÃ³n de Archivos\")\n",
    "        \n",
    "        data_dir = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "        if os.path.exists(data_dir):\n",
    "            files = os.listdir(data_dir)\n",
    "            csv_files = [f for f in files if f.endswith('.csv')]\n",
    "            \n",
    "            if csv_files:\n",
    "                st.success(f\"âœ… Directorio data/raw/ encontrado\")\n",
    "                st.write(f\"ğŸ“„ Archivos CSV: {csv_files}\")\n",
    "            else:\n",
    "                st.warning(f\"âš ï¸ Directorio existe pero no hay archivos CSV\")\n",
    "        else:\n",
    "            st.error(f\"âŒ No existe el directorio: {data_dir}\")\n",
    "            st.info(\"\"\"\n",
    "            **SoluciÃ³n:**\n",
    "            1. Crea la carpeta `data/raw/` en tu proyecto\n",
    "            2. Coloca allÃ­ tus archivos `kepler.csv`, `k2.csv`, `tess.csv`\n",
    "            3. Recarga esta pÃ¡gina\n",
    "            \"\"\")\n",
    "        \n",
    "        # Verificar si hay modelo entrenado\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        if os.path.exists(model_path):\n",
    "            st.success(\"âœ… **Modelo entrenado disponible** - Puedes usarlo en 'Clasificar Exoplanetas'\")\n",
    "            if self.model.load_model(model_path):\n",
    "                st.metric(\"Accuracy del Modelo\", f\"{self.model.accuracy:.1%}\")\n",
    "                self.model_trained = True\n",
    "        else:\n",
    "            st.warning(\"âš ï¸ **No hay modelo entrenado** - Ve a 'Entrenar Modelo REAL' para comenzar\")\n",
    "\n",
    "    def render_real_training(self):\n",
    "        \"\"\"PÃ¡gina de entrenamiento REAL con datos de la NASA\"\"\"\n",
    "        st.title(\"ğŸš€ Entrenamiento REAL con Datos NASA\")\n",
    "        \n",
    "        st.info(\"\"\"\n",
    "        **Entrenamiento REAL del modelo Ensemble** usando tus datasets de:\n",
    "        - Kepler.csv (datos reales)\n",
    "        - K2.csv (datos reales) \n",
    "        - TESS.csv (datos reales)\n",
    "        \n",
    "        El modelo entrenado se guardarÃ¡ automÃ¡ticamente y estarÃ¡ disponible para clasificaciÃ³n.\n",
    "        \"\"\")\n",
    "        \n",
    "        if st.button(\"ğŸ¯ Iniciar Entrenamiento REAL\", type=\"primary\"):\n",
    "            with st.spinner(\"Cargando y procesando datos REALES de la NASA...\"):\n",
    "                try:\n",
    "                    # Cargar datos reales\n",
    "                    kepler_df, k2_df, tess_df = self.data_processor.load_real_data()\n",
    "                    \n",
    "                    if kepler_df is None:\n",
    "                        st.error(\"\"\"\n",
    "                        âŒ **No se pudieron cargar los datasets**\n",
    "                        \n",
    "                        **Posibles soluciones:**\n",
    "                        1. Verifica que los archivos estÃ©n en `data/raw/`\n",
    "                        2. AsegÃºrate de que se llamen `kepler.csv`, `k2.csv`, `tess.csv`\n",
    "                        3. Verifica que los archivos no estÃ©n corruptos\n",
    "                        \"\"\")\n",
    "                        return\n",
    "                    \n",
    "                    # Mostrar informaciÃ³n de los datasets\n",
    "                    st.subheader(\"ğŸ“Š Datasets Cargados\")\n",
    "                    col1, col2, col3 = st.columns(3)\n",
    "                    \n",
    "                    with col1:\n",
    "                        st.metric(\"Kepler\", f\"{len(kepler_df):,} registros\")\n",
    "                    with col2:\n",
    "                        k2_count = len(k2_df) if k2_df is not None else 0\n",
    "                        st.metric(\"K2\", f\"{k2_count:,} registros\")\n",
    "                    with col3:\n",
    "                        tess_count = len(tess_df) if tess_df is not None else 0\n",
    "                        st.metric(\"TESS\", f\"{tess_count:,} registros\")\n",
    "                    \n",
    "                    # Procesar datos\n",
    "                    st.subheader(\"ğŸ”§ Procesando Datos...\")\n",
    "                    \n",
    "                    # Preprocesar Kepler\n",
    "                    kepler_processed = self.data_processor.preprocess_kepler(kepler_df)\n",
    "                    if kepler_processed is None:\n",
    "                        return\n",
    "                    \n",
    "                    # Preprocesar K2 y TESS si estÃ¡n disponibles\n",
    "                    datasets_to_process = [kepler_processed]\n",
    "                    \n",
    "                    if k2_df is not None:\n",
    "                        k2_processed = self.data_processor.preprocess_k2(k2_df)\n",
    "                        if k2_processed is not None:\n",
    "                            datasets_to_process.append(k2_processed)\n",
    "                    \n",
    "                    if tess_df is not None:\n",
    "                        tess_processed = self.data_processor.preprocess_tess(tess_df)\n",
    "                        if tess_processed is not None:\n",
    "                            datasets_to_process.append(tess_processed)\n",
    "                    \n",
    "                    # Unificar datos (filtrar None values)\n",
    "                    datasets_to_process = [d for d in datasets_to_process if d is not None]\n",
    "                    if not datasets_to_process:\n",
    "                        st.error(\"âŒ No hay datos vÃ¡lidos para procesar\")\n",
    "                        return\n",
    "                    \n",
    "                    unified_data = pd.concat(datasets_to_process, ignore_index=True)\n",
    "                    \n",
    "                    st.success(f\"âœ… Datos unificados: {len(unified_data):,} muestras\")\n",
    "                    \n",
    "                    # Preparar caracterÃ­sticas\n",
    "                    X, y, feature_names = self.data_processor.prepare_features(unified_data)\n",
    "                    \n",
    "                    if X is None:\n",
    "                        st.error(\"âŒ No se pudieron preparar las caracterÃ­sticas\")\n",
    "                        return\n",
    "                    \n",
    "                    # Entrenar modelo\n",
    "                    st.subheader(\"ğŸ¤– Entrenando Modelo Ensemble...\")\n",
    "                    trained_model = self.model.train(X, y)\n",
    "                    \n",
    "                    if trained_model is None:\n",
    "                        st.error(\"âŒ Error en el entrenamiento\")\n",
    "                        return\n",
    "                    \n",
    "                    # Guardar modelo\n",
    "                    models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "                    model_path = os.path.join(models_dir, 'real_ensemble_model.pkl')\n",
    "                    \n",
    "                    model_saved = self.model.save_model(model_path)\n",
    "                    \n",
    "                    if model_saved:\n",
    "                        # Guardar tambiÃ©n el preprocesador y feature names\n",
    "                        processor_path = os.path.join(models_dir, 'data_processor.pkl')\n",
    "                        features_path = os.path.join(models_dir, 'feature_names.pkl')\n",
    "                        \n",
    "                        joblib.dump(self.data_processor, processor_path)\n",
    "                        joblib.dump(feature_names, features_path)\n",
    "                        \n",
    "                        st.success(\"âœ… Modelo entrenado y guardado exitosamente!\")\n",
    "                        self.model_trained = True\n",
    "                        \n",
    "                        # Mostrar resultados\n",
    "                        st.subheader(\"ğŸ“ˆ Resultados del Entrenamiento\")\n",
    "                        col1, col2, col3, col4 = st.columns(4)\n",
    "                        \n",
    "                        with col1:\n",
    "                            st.metric(\"Accuracy\", f\"{self.model.accuracy:.2%}\")\n",
    "                        with col2:\n",
    "                            st.metric(\"Muestras\", f\"{X.shape[0]:,}\")\n",
    "                        with col3:\n",
    "                            st.metric(\"CaracterÃ­sticas\", X.shape[1])\n",
    "                        with col4:\n",
    "                            st.metric(\"Algoritmos\", \"4 Ensemble\")\n",
    "                        \n",
    "                        # Importancia de caracterÃ­sticas\n",
    "                        if self.model.feature_importance is not None:\n",
    "                            st.subheader(\"ğŸ” Importancia de CaracterÃ­sticas\")\n",
    "                            importance_df = pd.DataFrame({\n",
    "                                'CaracterÃ­stica': feature_names,\n",
    "                                'Importancia': self.model.feature_importance\n",
    "                            }).sort_values('Importancia', ascending=False)\n",
    "                            \n",
    "                            fig = px.bar(\n",
    "                                importance_df.head(10),\n",
    "                                x='Importancia',\n",
    "                                y='CaracterÃ­stica',\n",
    "                                title='Top 10 CaracterÃ­sticas MÃ¡s Importantes',\n",
    "                                orientation='h'\n",
    "                            )\n",
    "                            st.plotly_chart(fig, use_container_width=True)\n",
    "                    \n",
    "                    st.balloons()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    st.error(f\"âŒ Error durante el entrenamiento: {str(e)}\")\n",
    "                    import traceback\n",
    "                    st.code(traceback.format_exc())\n",
    "\n",
    "    # ... (el resto de las funciones se mantienen igual que antes)\n",
    "    def render_real_classification(self):\n",
    "        \"\"\"ClasificaciÃ³n con modelo REAL entrenado - VERSIÃ“N CORREGIDA\"\"\"\n",
    "        st.title(\"ğŸ¤– ClasificaciÃ³n con Modelo REAL\")\n",
    "        \n",
    "        # Verificar si hay modelo entrenado\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        if not os.path.exists(model_path):\n",
    "            st.warning(\"\"\"\n",
    "            âš ï¸ **No hay modelo entrenado**\n",
    "            \n",
    "            Para usar el clasificador REAL:\n",
    "            1. Ve a la pestaÃ±a **'Entrenar Modelo REAL'**\n",
    "            2. Entrena el modelo con tus datos de la NASA\n",
    "            3. Regresa aquÃ­ para clasificar candidatos\n",
    "            \"\"\")\n",
    "            return\n",
    "        \n",
    "        # Cargar modelo\n",
    "        if not self.model_trained:\n",
    "            if self.model.load_model(model_path):\n",
    "                self.model_trained = True\n",
    "                st.success(\"âœ… Modelo REAL cargado exitosamente\")\n",
    "                \n",
    "                # Mostrar informaciÃ³n de caracterÃ­sticas\n",
    "                features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "                if os.path.exists(features_path):\n",
    "                    feature_names = joblib.load(features_path)\n",
    "                    st.info(f\"ğŸ” El modelo espera {len(feature_names)} caracterÃ­sticas: {', '.join(feature_names)}\")\n",
    "            else:\n",
    "                st.error(\"âŒ Error cargando el modelo\")\n",
    "                return\n",
    "        \n",
    "        st.info(\"\"\"\n",
    "        ğŸ” **Clasificador REAL**: Introduce los 9 parÃ¡metros astronÃ³micos que el modelo espera.\n",
    "        \"\"\")\n",
    "        \n",
    "        with st.form(\"real_classification_form\"):\n",
    "            st.subheader(\"ğŸ“ ParÃ¡metros del Candidato - 9 CARACTERÃSTICAS REQUERIDAS\")\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                koi_period = st.number_input(\"PerÃ­odo Orbital - koi_period (dÃ­as)\", \n",
    "                                        min_value=0.1, max_value=1000.0, value=10.0,\n",
    "                                        help=\"Tiempo orbital del planeta\")\n",
    "                \n",
    "                koi_duration = st.number_input(\"DuraciÃ³n TrÃ¡nsito - koi_duration (horas)\", \n",
    "                                            min_value=0.1, max_value=24.0, value=3.0,\n",
    "                                            help=\"DuraciÃ³n del trÃ¡nsito\")\n",
    "                \n",
    "                koi_depth = st.number_input(\"Profundidad TrÃ¡nsito - koi_depth (ppm)\", \n",
    "                                        min_value=1, max_value=100000, value=500,\n",
    "                                        help=\"DisminuciÃ³n de brillo durante trÃ¡nsito\")\n",
    "                \n",
    "                koi_prad = st.number_input(\"Radio Planetario - koi_prad (Radios Tierra)\", \n",
    "                                        min_value=0.1, max_value=50.0, value=2.0,\n",
    "                                        help=\"Radio del planeta en unidades terrestres\")\n",
    "                \n",
    "                koi_teq = st.number_input(\"Temperatura Equilibrio - koi_teq (K)\", \n",
    "                                        min_value=100, max_value=5000, value=500,\n",
    "                                        help=\"Temperatura de equilibrio del planeta\")\n",
    "            \n",
    "            with col2:\n",
    "                koi_insol = st.number_input(\"Flujo de InsolaciÃ³n - koi_insol\", \n",
    "                                        min_value=0.1, max_value=10000.0, value=100.0,\n",
    "                                        help=\"Flujo de radiaciÃ³n recibido\")\n",
    "                \n",
    "                koi_steff = st.number_input(\"Temperatura Estelar - koi_steff (K)\", \n",
    "                                        min_value=2000, max_value=15000, value=5800,\n",
    "                                        help=\"Temperatura efectiva de la estrella\")\n",
    "                \n",
    "                koi_slogg = st.number_input(\"Gravedad Estelar - koi_slogg (log g)\", \n",
    "                                        min_value=3.0, max_value=5.5, value=4.4,\n",
    "                                        help=\"Gravedad superficial estelar\")\n",
    "                \n",
    "                # Â¡ESTE ES EL CAMPO QUE FALTABA!\n",
    "                koi_srad = st.number_input(\"Radio Estelar - koi_srad (Radios Sol)\", \n",
    "                                        min_value=0.1, max_value=10.0, value=1.0,\n",
    "                                        help=\"Radio de la estrella en unidades solares\")\n",
    "            \n",
    "            submitted = st.form_submit_button(\"ğŸš€ Clasificar con Modelo REAL\")\n",
    "        \n",
    "        if submitted:\n",
    "            # Verificar que tenemos todas las caracterÃ­sticas\n",
    "            features = (\n",
    "                koi_period, koi_duration, koi_depth, koi_prad,\n",
    "                koi_teq, koi_insol, koi_steff, koi_slogg, koi_srad  # Â¡Ahora son 9!\n",
    "            )\n",
    "            \n",
    "            st.info(f\"ğŸ” Enviando {len(features)} caracterÃ­sticas al modelo\")\n",
    "            self._real_prediction(*features)\n",
    "\n",
    "    def _real_prediction(self, *features):\n",
    "        \"\"\"PredicciÃ³n REAL con el modelo entrenado - VERSIÃ“N CORREGIDA\"\"\"\n",
    "        # Cargar informaciÃ³n del modelo\n",
    "        processor_path = os.path.join(PROJECT_ROOT, 'models', 'data_processor.pkl')\n",
    "        features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "        \n",
    "        try:\n",
    "            # Verificar que tenemos los archivos necesarios\n",
    "            if not os.path.exists(processor_path) or not os.path.exists(features_path):\n",
    "                st.error(\"âŒ No se encontraron los archivos del modelo entrenado\")\n",
    "                return\n",
    "            \n",
    "            # Cargar feature names y preprocesador\n",
    "            saved_feature_names = joblib.load(features_path)\n",
    "            data_processor = joblib.load(processor_path)\n",
    "            \n",
    "            # VERIFICACIÃ“N CRÃTICA: Â¿Coincide el nÃºmero de caracterÃ­sticas?\n",
    "            if len(features) != len(saved_feature_names):\n",
    "                st.error(f\"\"\"\n",
    "                âŒ **ERROR CRÃTICO - Discrepancia en caracterÃ­sticas**\n",
    "                \n",
    "                **EnvÃ­as:** {len(features)} caracterÃ­sticas\n",
    "                **Modelo espera:** {len(saved_feature_names)} caracterÃ­sticas\n",
    "                \n",
    "                **CaracterÃ­sticas esperadas por el modelo:**\n",
    "                {saved_feature_names}\n",
    "                \n",
    "                **SoluciÃ³n:** AsegÃºrate de que el formulario tenga exactamente {len(saved_feature_names)} campos.\n",
    "                \"\"\")\n",
    "                return\n",
    "            \n",
    "            st.success(f\"âœ… Coincidencia perfecta: {len(features)} caracterÃ­sticas enviadas\")\n",
    "            \n",
    "            # Crear array de caracterÃ­sticas\n",
    "            feature_array = np.array([features]).reshape(1, -1)\n",
    "            \n",
    "            # Escalar caracterÃ­sticas\n",
    "            feature_array_scaled = data_processor.scaler.transform(feature_array)\n",
    "            \n",
    "            # Realizar predicciÃ³n\n",
    "            prediction = self.model.model.predict(feature_array_scaled)[0]\n",
    "            probability = self.model.model.predict_proba(feature_array_scaled)[0, 1]\n",
    "            \n",
    "            # Mostrar resultados\n",
    "            st.subheader(\"ğŸ¯ Resultado de la ClasificaciÃ³n REAL\")\n",
    "            \n",
    "            col1, col2 = st.columns([1, 2])\n",
    "            \n",
    "            with col1:\n",
    "                if prediction == 1:\n",
    "                    st.success(\"âœ… **EXOPLANETA DETECTADO**\")\n",
    "                    st.balloons()\n",
    "                else:\n",
    "                    st.error(\"âŒ **NO ES EXOPLANETA**\")\n",
    "                \n",
    "                st.metric(\"Probabilidad\", f\"{probability:.2%}\")\n",
    "                \n",
    "                # InterpretaciÃ³n de la probabilidad\n",
    "                if probability >= 0.8:\n",
    "                    st.info(\"ğŸŸ¢ **Alta confianza** - Muy probable exoplaneta\")\n",
    "                elif probability >= 0.6:\n",
    "                    st.info(\"ğŸŸ¡ **Confianza media** - Posible exoplaneta\")\n",
    "                else:\n",
    "                    st.info(\"ğŸ”´ **Baja confianza** - Probable falso positivo\")\n",
    "            \n",
    "            with col2:\n",
    "                # AnÃ¡lisis detallado de caracterÃ­sticas\n",
    "                st.markdown(\"#### ğŸ“Š AnÃ¡lisis de CaracterÃ­sticas\")\n",
    "                \n",
    "                # Mapeo de nombres amigables\n",
    "                feature_display_names = {\n",
    "                    'koi_period': 'PerÃ­odo Orbital',\n",
    "                    'koi_duration': 'DuraciÃ³n TrÃ¡nsito', \n",
    "                    'koi_depth': 'Profundidad TrÃ¡nsito',\n",
    "                    'koi_prad': 'Radio Planetario',\n",
    "                    'koi_teq': 'Temperatura Planeta',\n",
    "                    'koi_insol': 'Flujo InsolaciÃ³n',\n",
    "                    'koi_steff': 'Temperatura Estelar',\n",
    "                    'koi_slogg': 'Gravedad Estelar',\n",
    "                    'koi_srad': 'Radio Estelar'\n",
    "                }\n",
    "                \n",
    "                # Mapeo de unidades\n",
    "                feature_units = {\n",
    "                    'koi_period': 'dÃ­as',\n",
    "                    'koi_duration': 'horas', \n",
    "                    'koi_depth': 'ppm',\n",
    "                    'koi_prad': 'RâŠ•',\n",
    "                    'koi_teq': 'K',\n",
    "                    'koi_insol': 'SâŠ•',\n",
    "                    'koi_steff': 'K',\n",
    "                    'koi_slogg': 'log g',\n",
    "                    'koi_srad': 'Râ˜‰'\n",
    "                }\n",
    "                \n",
    "                # Crear tabla de anÃ¡lisis\n",
    "                analysis_data = []\n",
    "                for i, feature_name in enumerate(saved_feature_names):\n",
    "                    display_name = feature_display_names.get(feature_name, feature_name)\n",
    "                    units = feature_units.get(feature_name, '')\n",
    "                    value = features[i]\n",
    "                    \n",
    "                    analysis_data.append({\n",
    "                        'CaracterÃ­stica': display_name,\n",
    "                        'Valor': f\"{value} {units}\",\n",
    "                        'CÃ³digo': feature_name\n",
    "                    })\n",
    "                \n",
    "                analysis_df = pd.DataFrame(analysis_data)\n",
    "                st.dataframe(analysis_df, use_container_width=True, hide_index=True)\n",
    "                \n",
    "                # InformaciÃ³n adicional\n",
    "                st.markdown(\"#### ğŸ’¡ InformaciÃ³n del Modelo\")\n",
    "                st.info(f\"\"\"\n",
    "                - **Modelo:** Ensemble Stacking (4 algoritmos)\n",
    "                - **CaracterÃ­sticas:** {len(saved_feature_names)}\n",
    "                - **PrecisiÃ³n:** ~83%\n",
    "                - **Datos de entrenamiento:** Kepler + K2 + TESS (NASA)\n",
    "                \"\"\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"âŒ Error en la predicciÃ³n: {e}\")\n",
    "            st.info(\"ğŸ’¡ **SoluciÃ³n:** Reentrena el modelo en la pestaÃ±a 'Entrenar Modelo REAL'\")\n",
    "\n",
    "    def render_real_analysis(self):\n",
    "        \"\"\"AnÃ¡lisis de datos REALES\"\"\"\n",
    "        st.title(\"ğŸ“Š AnÃ¡lisis de Datos REALES NASA\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar datos reales\n",
    "            kepler_df, k2_df, tess_df = self.data_processor.load_real_data()\n",
    "            \n",
    "            if kepler_df is None:\n",
    "                st.warning(\"No se pudieron cargar los datasets para anÃ¡lisis\")\n",
    "                return\n",
    "            \n",
    "            st.success(f\"âœ… Datasets cargados: Kepler ({len(kepler_df):,}), K2 ({len(k2_df):,}), TESS ({len(tess_df):,})\")\n",
    "            \n",
    "            # Selector de dataset\n",
    "            dataset_choice = st.selectbox(\"Seleccionar Dataset para AnÃ¡lisis:\", \n",
    "                                        [\"Kepler\", \"K2\", \"TESS\"])\n",
    "            \n",
    "            if dataset_choice == \"Kepler\":\n",
    "                df = kepler_df\n",
    "                st.subheader(\"ğŸ”­ Dataset Kepler\")\n",
    "            elif dataset_choice == \"K2\":\n",
    "                df = k2_df\n",
    "                st.subheader(\"ğŸ›°ï¸ Dataset K2\")\n",
    "            else:\n",
    "                df = tess_df\n",
    "                st.subheader(\"ğŸ“¡ Dataset TESS\")\n",
    "            \n",
    "            # Mostrar informaciÃ³n bÃ¡sica\n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            \n",
    "            with col1:\n",
    "                st.metric(\"Total Registros\", f\"{len(df):,}\")\n",
    "            with col2:\n",
    "                st.metric(\"Columnas\", df.shape[1])\n",
    "            with col3:\n",
    "                missing = df.isnull().sum().sum()\n",
    "                st.metric(\"Valores Missing\", f\"{missing:,}\")\n",
    "            \n",
    "            # Vista previa de datos\n",
    "            st.subheader(\"ğŸ‘€ Vista Previa de Datos\")\n",
    "            st.dataframe(df.head(10), use_container_width=True)\n",
    "            \n",
    "            # AnÃ¡lisis de columnas\n",
    "            st.subheader(\"ğŸ“‹ Columnas Disponibles\")\n",
    "            st.write(f\"Total de columnas: {len(df.columns)}\")\n",
    "            st.write(list(df.columns))\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error en el anÃ¡lisis: {e}\")\n",
    "\n",
    "    def render_saved_models(self):\n",
    "        \"\"\"GestiÃ³n de modelos guardados - VERSIÃ“N MEJORADA\"\"\"\n",
    "        st.title(\"ğŸ’¾ Modelos Guardados\")\n",
    "        \n",
    "        models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "        \n",
    "        # Crear la carpeta si no existe\n",
    "        if not os.path.exists(models_dir):\n",
    "            st.warning(\"ğŸ“ La carpeta de modelos no existe. CreÃ¡ndola...\")\n",
    "            os.makedirs(models_dir, exist_ok=True)\n",
    "            st.success(f\"âœ… Carpeta creada: {models_dir}\")\n",
    "        \n",
    "        # Verificar archivos en la carpeta\n",
    "        try:\n",
    "            model_files = [f for f in os.listdir(models_dir) if f.endswith('.pkl')]\n",
    "        except FileNotFoundError:\n",
    "            model_files = []\n",
    "        \n",
    "        if not model_files:\n",
    "            st.info(\"\"\"\n",
    "            ğŸ“­ **No hay modelos guardados**\n",
    "            \n",
    "            Los modelos aparecerÃ¡n aquÃ­ despuÃ©s de:\n",
    "            1. ğŸš€ Entrenar un modelo en la pestaÃ±a \"Entrenar Modelo REAL\"\n",
    "            2. ğŸ’¾ El modelo se guardarÃ¡ automÃ¡ticamente en la carpeta `models/`\n",
    "            \n",
    "            **Archivos que se guardan:**\n",
    "            - `real_ensemble_model.pkl` - Modelo ensemble principal\n",
    "            - `data_processor.pkl` - Preprocesador de datos  \n",
    "            - `feature_names.pkl` - Nombres de caracterÃ­sticas\n",
    "            \"\"\")\n",
    "            \n",
    "            # Mostrar estructura esperada\n",
    "            st.subheader(\"ğŸ“ Estructura esperada:\")\n",
    "            st.code(\"\"\"\n",
    "            exoplanet-ai-detector/\n",
    "            â”œâ”€â”€ models/\n",
    "            â”‚   â”œâ”€â”€ real_ensemble_model.pkl\n",
    "            â”‚   â”œâ”€â”€ data_processor.pkl  \n",
    "            â”‚   â””â”€â”€ feature_names.pkl\n",
    "            â”œâ”€â”€ data/\n",
    "            â”‚   â””â”€â”€ raw/\n",
    "            â”‚       â”œâ”€â”€ kepler.csv\n",
    "            â”‚       â”œâ”€â”€ k2.csv\n",
    "            â”‚       â””â”€â”€ tess.csv\n",
    "            â””â”€â”€ webapp/\n",
    "                â””â”€â”€ app.py\n",
    "            \"\"\")\n",
    "            return\n",
    "        \n",
    "        st.success(f\"âœ… Se encontraron {len(model_files)} modelos guardados\")\n",
    "        \n",
    "        # Mostrar modelos disponibles\n",
    "        st.subheader(\"ğŸ“ Modelos Disponibles\")\n",
    "        \n",
    "        for model_file in model_files:\n",
    "            file_path = os.path.join(models_dir, model_file)\n",
    "            file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "            file_time = os.path.getmtime(file_path)\n",
    "            from datetime import datetime\n",
    "            file_date = datetime.fromtimestamp(file_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            col1, col2, col3, col4, col5 = st.columns([3, 1, 1, 1, 1])\n",
    "            \n",
    "            with col1:\n",
    "                # Icono diferente segÃºn el tipo de archivo\n",
    "                if \"ensemble\" in model_file:\n",
    "                    icon = \"ğŸ¤–\"\n",
    "                elif \"processor\" in model_file:\n",
    "                    icon = \"ğŸ”§\" \n",
    "                elif \"feature\" in model_file:\n",
    "                    icon = \"ğŸ“Š\"\n",
    "                else:\n",
    "                    icon = \"ğŸ“„\"\n",
    "                    \n",
    "                st.write(f\"{icon} **{model_file}**\")\n",
    "                st.caption(f\"Creado: {file_date}\")\n",
    "                \n",
    "            with col2:\n",
    "                st.write(f\"{file_size:.1f} MB\")\n",
    "                \n",
    "            with col3:\n",
    "                # BotÃ³n de informaciÃ³n\n",
    "                if st.button(\"â„¹ï¸\", key=f\"info_{model_file}\", help=\"Ver informaciÃ³n\"):\n",
    "                    self._show_model_info(model_file, file_path)\n",
    "                    \n",
    "            with col4:\n",
    "                # BotÃ³n de carga\n",
    "                if st.button(\"ğŸ“¥\", key=f\"load_{model_file}\", help=\"Cargar modelo\"):\n",
    "                    if self._load_specific_model(model_file):\n",
    "                        st.success(f\"âœ… {model_file} cargado\")\n",
    "                        self.model_trained = True\n",
    "                    else:\n",
    "                        st.error(f\"âŒ Error cargando {model_file}\")\n",
    "                        \n",
    "            with col5:\n",
    "                # BotÃ³n de eliminaciÃ³n con confirmaciÃ³n\n",
    "                if st.button(\"ğŸ—‘ï¸\", key=f\"delete_{model_file}\", help=\"Eliminar modelo\"):\n",
    "                    if st.checkbox(f\"Â¿Confirmar eliminaciÃ³n de {model_file}?\", key=f\"confirm_{model_file}\"):\n",
    "                        try:\n",
    "                            os.remove(file_path)\n",
    "                            st.success(f\"âœ… {model_file} eliminado\")\n",
    "                            st.rerun()\n",
    "                        except Exception as e:\n",
    "                            st.error(f\"âŒ Error eliminando: {e}\")\n",
    "        \n",
    "        # EstadÃ­sticas de la carpeta\n",
    "        st.subheader(\"ğŸ“ˆ EstadÃ­sticas de Modelos\")\n",
    "        total_size = sum(os.path.getsize(os.path.join(models_dir, f)) for f in model_files) / 1024 / 1024\n",
    "        \n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        with col1:\n",
    "            st.metric(\"Total Modelos\", len(model_files))\n",
    "        with col2:\n",
    "            st.metric(\"Espacio Total\", f\"{total_size:.1f} MB\")\n",
    "        with col3:\n",
    "            st.metric(\"Carpeta\", \"models/\")\n",
    "        \n",
    "        # InformaciÃ³n de uso\n",
    "        st.subheader(\"â„¹ï¸ InformaciÃ³n de Archivos\")\n",
    "        st.info(\"\"\"\n",
    "        **Archivos del sistema:**\n",
    "        - ğŸ¤– **real_ensemble_model.pkl** - Modelo ensemble principal (Random Forest + XGBoost + LightGBM + Extra Trees)\n",
    "        - ğŸ”§ **data_processor.pkl** - Preprocesador con scaler y configuraciÃ³n de caracterÃ­sticas\n",
    "        - ğŸ“Š **feature_names.pkl** - Nombres y orden de las caracterÃ­sticas usadas en el entrenamiento\n",
    "        \n",
    "        **Recomendaciones:**\n",
    "        - No elimines archivos manualmente desde el sistema de archivos\n",
    "        - Usa los botones de esta interfaz para gestiÃ³n segura\n",
    "        - Los 3 archivos deben estar presentes para que el sistema funcione correctamente\n",
    "        \"\"\")\n",
    "        \n",
    "        # BotÃ³n para crear modelo de ejemplo (para testing)\n",
    "        st.subheader(\"ğŸ› ï¸ Herramientas\")\n",
    "        if st.button(\"ğŸ§ª Crear Modelo de Ejemplo\", help=\"Crear un modelo dummy para testing\"):\n",
    "            self._create_example_model()\n",
    "            \n",
    "        if st.button(\"ğŸ”„ Actualizar Lista\", help=\"Refrescar la lista de modelos\"):\n",
    "            st.rerun()\n",
    "\n",
    "    def _show_model_info(self, model_file, file_path):\n",
    "        \"\"\"Mostrar informaciÃ³n detallada de un modelo\"\"\"\n",
    "        try:\n",
    "            if \"ensemble\" in model_file:\n",
    "                model = joblib.load(file_path)\n",
    "                st.info(f\"\"\"\n",
    "                **ğŸ¤– Modelo Ensemble: {model_file}**\n",
    "                \n",
    "                - **Tipo:** StackingClassifier\n",
    "                - **Algoritmos base:** {len(model.named_estimators_)}\n",
    "                - **Estimadores:** {list(model.named_estimators_.keys())}\n",
    "                - **Meta-estimador:** {type(model.final_estimator_).__name__}\n",
    "                \"\"\")\n",
    "                \n",
    "            elif \"processor\" in model_file:\n",
    "                processor = joblib.load(file_path)\n",
    "                st.info(f\"\"\"\n",
    "                **ğŸ”§ Preprocesador: {model_file}**\n",
    "                \n",
    "                - **Tipo:** ExoplanetDataProcessor\n",
    "                - **CaracterÃ­sticas escaladas:** {len(processor.feature_names) if hasattr(processor, 'feature_names') else 'N/A'}\n",
    "                - **Scaler:** {type(processor.scaler).__name__ if hasattr(processor, 'scaler') else 'N/A'}\n",
    "                \"\"\")\n",
    "                \n",
    "            elif \"feature\" in model_file:\n",
    "                features = joblib.load(file_path)\n",
    "                st.info(f\"\"\"\n",
    "                **ğŸ“Š CaracterÃ­sticas: {model_file}**\n",
    "                \n",
    "                - **NÃºmero de caracterÃ­sticas:** {len(features)}\n",
    "                - **CaracterÃ­sticas:** {features}\n",
    "                \"\"\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"âŒ Error cargando informaciÃ³n de {model_file}: {e}\")\n",
    "\n",
    "    def _load_specific_model(self, model_file):\n",
    "        \"\"\"Cargar un modelo especÃ­fico\"\"\"\n",
    "        try:\n",
    "            model_path = os.path.join(PROJECT_ROOT, 'models', model_file)\n",
    "            \n",
    "            if \"ensemble\" in model_file:\n",
    "                return self.model.load_model(model_path)\n",
    "            else:\n",
    "                st.info(f\"ğŸ“¥ {model_file} cargado (no es el modelo principal)\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error cargando {model_file}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _create_example_model(self):\n",
    "        \"\"\"Crear un modelo de ejemplo para testing\"\"\"\n",
    "        try:\n",
    "            models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "            os.makedirs(models_dir, exist_ok=True)\n",
    "            \n",
    "            # Crear feature names de ejemplo\n",
    "            feature_names = ['koi_period', 'koi_duration', 'koi_depth', 'koi_prad', \n",
    "                            'koi_teq', 'koi_insol', 'koi_steff', 'koi_slogg', 'koi_srad']\n",
    "            joblib.dump(feature_names, os.path.join(models_dir, 'feature_names.pkl'))\n",
    "            \n",
    "            # Crear processor de ejemplo\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            class ExampleProcessor:\n",
    "                def __init__(self):\n",
    "                    self.scaler = StandardScaler()\n",
    "                    self.feature_names = feature_names\n",
    "            processor = ExampleProcessor()\n",
    "            joblib.dump(processor, os.path.join(models_dir, 'data_processor.pkl'))\n",
    "            \n",
    "            st.success(\"âœ… Modelo de ejemplo creado para testing\")\n",
    "            st.rerun()\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"âŒ Error creando modelo de ejemplo: {e}\")\n",
    "\n",
    "    def render_batch_classification(self):\n",
    "        \"\"\"ClasificaciÃ³n por lotes de archivos CSV completos\"\"\"\n",
    "        st.title(\"ğŸ“¦ ClasificaciÃ³n por Lotes\")\n",
    "        \n",
    "        st.markdown(\"\"\"\n",
    "        ### ğŸš€ ClasificaciÃ³n Masiva de Exoplanetas\n",
    "        \n",
    "        **Sube un archivo CSV completo** (como Kepler, K2 o TESS) y el sistema:\n",
    "        - âœ… ClasificarÃ¡ automÃ¡ticamente todos los candidatos\n",
    "        - ğŸ“Š MostrarÃ¡ estadÃ­sticas completas\n",
    "        - ğŸ” IdentificarÃ¡ exoplanetas detectados\n",
    "        - ğŸ’¾ PermitirÃ¡ descargar resultados\n",
    "        \n",
    "        **Formatos compatibles:** Kepler, K2, TESS o cualquier CSV con las 9 caracterÃ­sticas requeridas\n",
    "        \"\"\")\n",
    "        \n",
    "        # Verificar si hay modelo entrenado\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        if not os.path.exists(model_path):\n",
    "            st.error(\"\"\"\n",
    "            âŒ **No hay modelo entrenado**\n",
    "            \n",
    "            Para usar la clasificaciÃ³n por lotes:\n",
    "            1. Ve a la pestaÃ±a **'Entrenar Modelo REAL'**\n",
    "            2. Entrena el modelo con tus datos de la NASA\n",
    "            3. Regresa aquÃ­ para clasificar archivos completos\n",
    "            \"\"\")\n",
    "            return\n",
    "        \n",
    "        # Cargar modelo si no estÃ¡ cargado\n",
    "        if not self.model_trained:\n",
    "            if self.model.load_model(model_path):\n",
    "                self.model_trained = True\n",
    "                st.success(\"âœ… Modelo REAL cargado exitosamente\")\n",
    "            else:\n",
    "                st.error(\"âŒ Error cargando el modelo\")\n",
    "                return\n",
    "        \n",
    "        # SecciÃ³n de carga de archivos\n",
    "        st.subheader(\"ğŸ“¤ Cargar Archivo CSV\")\n",
    "        \n",
    "        uploaded_file = st.file_uploader(\n",
    "            \"Selecciona un archivo CSV para clasificar\", \n",
    "            type=['csv'],\n",
    "            help=\"Archivos compatibles: Kepler, K2, TESS o cualquier CSV con las caracterÃ­sticas requeridas\"\n",
    "        )\n",
    "        \n",
    "        if uploaded_file is not None:\n",
    "            try:\n",
    "                # Leer el archivo\n",
    "                df = pd.read_csv(uploaded_file)\n",
    "                st.success(f\"âœ… Archivo cargado: {uploaded_file.name}\")\n",
    "                st.info(f\"ğŸ“Š Datos: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "                \n",
    "                # Mostrar vista previa\n",
    "                with st.expander(\"ğŸ‘€ Vista previa del archivo cargado\"):\n",
    "                    st.dataframe(df.head(10), use_container_width=True)\n",
    "                    st.write(f\"**Columnas disponibles:** {list(df.columns)}\")\n",
    "                \n",
    "                # Procesar el archivo\n",
    "                if st.button(\"ğŸ¯ Ejecutar ClasificaciÃ³n Masiva\", type=\"primary\"):\n",
    "                    with st.spinner(\"ğŸ” Clasificando candidatos... Esto puede tomar unos segundos\"):\n",
    "                        results = self._batch_predict(df, uploaded_file.name)\n",
    "                        \n",
    "                        if results is not None:\n",
    "                            self._display_batch_results(results, df)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                st.error(f\"âŒ Error procesando el archivo: {e}\")\n",
    "\n",
    "    def _batch_predict(self, df, filename):\n",
    "        \"\"\"Realizar predicciones por lotes\"\"\"\n",
    "        try:\n",
    "            # Cargar preprocesador y feature names\n",
    "            processor_path = os.path.join(PROJECT_ROOT, 'models', 'data_processor.pkl')\n",
    "            features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "            \n",
    "            if not os.path.exists(processor_path) or not os.path.exists(features_path):\n",
    "                st.error(\"âŒ No se encontraron los archivos del modelo entrenado\")\n",
    "                return None\n",
    "            \n",
    "            saved_feature_names = joblib.load(features_path)\n",
    "            data_processor = joblib.load(processor_path)\n",
    "            \n",
    "            st.info(f\"ğŸ” Modelo espera {len(saved_feature_names)} caracterÃ­sticas: {saved_feature_names}\")\n",
    "            \n",
    "            # Verificar que tenemos las caracterÃ­sticas necesarias\n",
    "            missing_features = [f for f in saved_feature_names if f not in df.columns]\n",
    "            if missing_features:\n",
    "                st.error(f\"âŒ Faltan caracterÃ­sticas en el archivo: {missing_features}\")\n",
    "                st.info(\"ğŸ’¡ **SoluciÃ³n:** AsegÃºrate de que el CSV tenga las mismas columnas que los datos de entrenamiento\")\n",
    "                return None\n",
    "            \n",
    "            # Seleccionar y preparar caracterÃ­sticas\n",
    "            X = df[saved_feature_names].copy()\n",
    "            \n",
    "            # Manejar valores missing\n",
    "            missing_before = X.isnull().sum().sum()\n",
    "            if missing_before > 0:\n",
    "                st.warning(f\"âš ï¸ Se encontraron {missing_before} valores missing. Imputando con medianas...\")\n",
    "                X = X.fillna(X.median())\n",
    "            \n",
    "            # Escalar caracterÃ­sticas\n",
    "            X_scaled = data_processor.scaler.transform(X)\n",
    "            \n",
    "            # Realizar predicciones\n",
    "            predictions = self.model.model.predict(X_scaled)\n",
    "            probabilities = self.model.model.predict_proba(X_scaled)[:, 1]\n",
    "            \n",
    "            # Crear DataFrame de resultados\n",
    "            results_df = df.copy()\n",
    "            results_df['prediction'] = predictions\n",
    "            results_df['probability'] = probabilities\n",
    "            results_df['classification'] = results_df['prediction'].map({1: 'EXOPLANETA', 0: 'NO_EXOPLANETA'})\n",
    "            \n",
    "            # AÃ±adir confianza\n",
    "            results_df['confidence'] = results_df['probability'].apply(\n",
    "                lambda x: 'ALTA' if x >= 0.8 else 'MEDIA' if x >= 0.6 else 'BAJA'\n",
    "            )\n",
    "            \n",
    "            return results_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"âŒ Error en clasificaciÃ³n por lotes: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _display_batch_results(self, results_df, original_df):\n",
    "        \"\"\"Mostrar resultados de clasificaciÃ³n por lotes\"\"\"\n",
    "        st.success(\"âœ… ClasificaciÃ³n completada exitosamente!\")\n",
    "        \n",
    "        # EstadÃ­sticas generales\n",
    "        total_candidates = len(results_df)\n",
    "        exoplanets_detected = results_df['prediction'].sum()\n",
    "        non_exoplanets = total_candidates - exoplanets_detected\n",
    "        \n",
    "        st.subheader(\"ğŸ“ˆ Resumen de ClasificaciÃ³n\")\n",
    "        \n",
    "        # MÃ©tricas principales\n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        \n",
    "        with col1:\n",
    "            st.metric(\"Total Candidatos\", total_candidates)\n",
    "        with col2:\n",
    "            st.metric(\"Exoplanetas Detectados\", exoplanets_detected)\n",
    "        with col3:\n",
    "            st.metric(\"No Exoplanetas\", non_exoplanets)\n",
    "        with col4:\n",
    "            detection_rate = (exoplanets_detected / total_candidates) * 100\n",
    "            st.metric(\"Tasa de DetecciÃ³n\", f\"{detection_rate:.1f}%\")\n",
    "        \n",
    "        # DistribuciÃ³n de confianza\n",
    "        confidence_counts = results_df['confidence'].value_counts()\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            # GrÃ¡fico de distribuciÃ³n\n",
    "            fig = px.pie(\n",
    "                results_df, \n",
    "                names='classification',\n",
    "                title='DistribuciÃ³n: Exoplanetas vs No Exoplanetas',\n",
    "                color='classification',\n",
    "                color_discrete_map={'EXOPLANETA': '#00CC96', 'NO_EXOPLANETA': '#EF553B'}\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        with col2:\n",
    "            # GrÃ¡fico de confianza\n",
    "            fig = px.bar(\n",
    "                confidence_counts,\n",
    "                title='DistribuciÃ³n por Nivel de Confianza',\n",
    "                labels={'index': 'Confianza', 'value': 'Cantidad'},\n",
    "                color=confidence_counts.index,\n",
    "                color_discrete_map={'ALTA': '#00CC96', 'MEDIA': '#FECB52', 'BAJA': '#EF553B'}\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        # Mostrar exoplanetas detectados\n",
    "        st.subheader(\"ğŸ” Exoplanetas Detectados\")\n",
    "        \n",
    "        exoplanets_df = results_df[results_df['prediction'] == 1].copy()\n",
    "        \n",
    "        if len(exoplanets_df) > 0:\n",
    "            st.success(f\"ğŸ¯ Se encontraron {len(exoplanets_df)} exoplanetas potenciales\")\n",
    "            \n",
    "            # Ordenar por probabilidad (mayor a menor)\n",
    "            exoplanets_df = exoplanets_df.sort_values('probability', ascending=False)\n",
    "            \n",
    "            # Seleccionar columnas importantes para mostrar\n",
    "            display_columns = []\n",
    "            possible_columns = [\n",
    "                'kepid', 'kepoi_name', 'kepler_name', 'koi_disposition',\n",
    "                'koi_period', 'koi_prad', 'koi_teq', 'probability', 'confidence'\n",
    "            ]\n",
    "            \n",
    "            for col in possible_columns:\n",
    "                if col in exoplanets_df.columns:\n",
    "                    display_columns.append(col)\n",
    "            \n",
    "            # AÃ±adir columnas de resultado si no estÃ¡n\n",
    "            if 'probability' not in display_columns:\n",
    "                display_columns.extend(['probability', 'confidence'])\n",
    "            \n",
    "            # Mostrar tabla de exoplanetas\n",
    "            st.dataframe(\n",
    "                exoplanets_df[display_columns].head(50),  # Mostrar mÃ¡ximo 50\n",
    "                use_container_width=True,\n",
    "                height=400\n",
    "            )\n",
    "            \n",
    "            # EstadÃ­sticas de los exoplanetas detectados\n",
    "            st.subheader(\"ğŸ“Š EstadÃ­sticas de Exoplanetas Detectados\")\n",
    "            \n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            \n",
    "            with col1:\n",
    "                avg_probability = exoplanets_df['probability'].mean()\n",
    "                st.metric(\"Probabilidad Promedio\", f\"{avg_probability:.1%}\")\n",
    "            \n",
    "            with col2:\n",
    "                high_confidence = len(exoplanets_df[exoplanets_df['confidence'] == 'ALTA'])\n",
    "                st.metric(\"Alta Confianza\", high_confidence)\n",
    "            \n",
    "            with col3:\n",
    "                if 'koi_prad' in exoplanets_df.columns:\n",
    "                    avg_radius = exoplanets_df['koi_prad'].mean()\n",
    "                    st.metric(\"Radio Promedio\", f\"{avg_radius:.1f} RâŠ•\")\n",
    "            \n",
    "            # DistribuciÃ³n de caracterÃ­sticas importantes\n",
    "            if 'koi_prad' in exoplanets_df.columns and 'koi_period' in exoplanets_df.columns:\n",
    "                st.subheader(\"ğŸ“ˆ CaracterÃ­sticas de Exoplanetas Detectados\")\n",
    "                \n",
    "                col1, col2 = st.columns(2)\n",
    "                \n",
    "                with col1:\n",
    "                    fig = px.histogram(\n",
    "                        exoplanets_df,\n",
    "                        x='koi_prad',\n",
    "                        title='DistribuciÃ³n de Radios Planetarios',\n",
    "                        labels={'koi_prad': 'Radio (Radios Tierra)'},\n",
    "                        nbins=20\n",
    "                    )\n",
    "                    st.plotly_chart(fig, use_container_width=True)\n",
    "                \n",
    "                with col2:\n",
    "                    fig = px.scatter(\n",
    "                        exoplanets_df,\n",
    "                        x='koi_period',\n",
    "                        y='probability',\n",
    "                        color='confidence',\n",
    "                        title='PerÃ­odo Orbital vs Probabilidad',\n",
    "                        labels={'koi_period': 'PerÃ­odo Orbital (dÃ­as)', 'probability': 'Probabilidad'},\n",
    "                        color_discrete_map={'ALTA': '#00CC96', 'MEDIA': '#FECB52', 'BAJA': '#EF553B'}\n",
    "                    )\n",
    "                    st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Descargar resultados\n",
    "            st.subheader(\"ğŸ’¾ Descargar Resultados\")\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                # Descargar solo exoplanetas\n",
    "                csv_exoplanets = exoplanets_df.to_csv(index=False)\n",
    "                st.download_button(\n",
    "                    label=\"ğŸ“¥ Descargar Exoplanetas Detectados\",\n",
    "                    data=csv_exoplanets,\n",
    "                    file_name=f\"exoplanetas_detectados_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.csv\",\n",
    "                    mime=\"text/csv\",\n",
    "                    help=\"Descargar solo los candidatos clasificados como exoplanetas\"\n",
    "                )\n",
    "            \n",
    "            with col2:\n",
    "                # Descargar todos los resultados\n",
    "                csv_all = results_df.to_csv(index=False)\n",
    "                st.download_button(\n",
    "                    label=\"ğŸ“¥ Descargar Todos los Resultados\",\n",
    "                    data=csv_all,\n",
    "                    file_name=f\"clasificacion_completa_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.csv\",\n",
    "                    mime=\"text/csv\",\n",
    "                    help=\"Descargar todos los candidatos con sus clasificaciones\"\n",
    "                )\n",
    "            \n",
    "            # InformaciÃ³n adicional\n",
    "            st.info(\"\"\"\n",
    "            ğŸ’¡ **InterpretaciÃ³n de resultados:**\n",
    "            - **ALTA confianza:** Probabilidad â‰¥ 80% - Muy probable exoplaneta\n",
    "            - **MEDIA confianza:** Probabilidad 60-79% - Posible exoplaneta  \n",
    "            - **BAJA confianza:** Probabilidad < 60% - Requiere mÃ¡s anÃ¡lisis\n",
    "            \"\"\")\n",
    "            \n",
    "        else:\n",
    "            st.warning(\"âš ï¸ No se detectaron exoplanetas en este archivo\")\n",
    "            \n",
    "            # Mostrar algunos candidatos con mayor probabilidad\n",
    "            top_candidates = results_df.nlargest(5, 'probability')\n",
    "            if len(top_candidates) > 0:\n",
    "                st.subheader(\"ğŸ¯ Candidatos MÃ¡s Prometedores\")\n",
    "                st.dataframe(\n",
    "                    top_candidates[['probability', 'confidence'] + \n",
    "                                [col for col in top_candidates.columns if col.startswith('koi_')][:5]],\n",
    "                    use_container_width=True\n",
    "                )\n",
    "\n",
    "    def _get_sample_datasets_info(self):\n",
    "        \"\"\"InformaciÃ³n sobre datasets de ejemplo\"\"\"\n",
    "        st.subheader(\"ğŸ› ï¸ Datasets de Prueba\")\n",
    "        \n",
    "        st.markdown(\"\"\"\n",
    "        **Puedes probar con estos datasets de ejemplo:**\n",
    "        \n",
    "        - **Kepler:** [Descargar de NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=cumulative)\n",
    "        - **K2:** [Descargar de NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=k2targets)\n",
    "        - **TESS:** [Descargar de NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=toi)\n",
    "        \n",
    "        **Estructura mÃ­nima requerida:**\n",
    "        ```csv\n",
    "        koi_period,koi_duration,koi_depth,koi_prad,koi_teq,koi_insol,koi_steff,koi_slogg,koi_srad\n",
    "        10.5,3.2,1500,2.1,450,95.0,5800,4.4,1.0\n",
    "        15.3,4.1,800,1.8,320,75.5,5200,4.5,0.9\n",
    "        ```\n",
    "        \"\"\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Ejecutar la aplicaciÃ³n completa - ACTUALIZADO\"\"\"\n",
    "        page = self.render_sidebar()\n",
    "        \n",
    "        if page == \"ğŸ  Inicio\":\n",
    "            self.render_home()\n",
    "        elif page == \"ğŸš€ Entrenar Modelo REAL\":\n",
    "            self.render_real_training()\n",
    "        elif page == \"ğŸ¤– Clasificar Exoplanetas\":\n",
    "            self.render_real_classification()\n",
    "        elif page == \"ğŸ“¦ ClasificaciÃ³n por Lotes\":  # Â¡NUEVA PÃGINA!\n",
    "            self.render_batch_classification()\n",
    "        elif page == \"ğŸ“Š AnÃ¡lisis de Datos REAL\":\n",
    "            self.render_real_analysis()\n",
    "        elif page == \"ğŸ’¾ Modelos Guardados\":\n",
    "            self.render_saved_models()\n",
    "\n",
    "# Ejecutar la aplicaciÃ³n\n",
    "if __name__ == \"__main__\":\n",
    "    app = ExoplanetDetectorApp()\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
